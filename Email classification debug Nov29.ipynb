{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov 27 09:43:55 2018\n",
    "\n",
    "@author: Anshuman_Mahapatra\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#import csv\n",
    "import string\n",
    "from scipy.sparse import lil_matrix, find\n",
    "import itertools\n",
    "#from pyjarowinkler import distance\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "import os.path\n",
    "import hashlib\n",
    "import pickle\n",
    "import filelock\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_df):\n",
    "        return data_df[self.key]\n",
    "    \n",
    "    def get_feature_names():\n",
    "       return []\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': (text.count('.') + \\\n",
    "                                   text.count('?') + \\\n",
    "                                   text.count('!'))}\n",
    "                for text in posts]\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return ['length','num_sentences']\n",
    "\n",
    "class TargetSimilarity(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, target,stop_words = None, ngram_range = (1,3),use_idf = False):\n",
    "        self.target = target\n",
    "        self.stop_words = stop_words\n",
    "        self.ngram_range = ngram_range\n",
    "        self.use_idf = use_idf\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text):\n",
    "        text_target = np.append(text,self.target)\n",
    "        count_vect = StemmedCountVectorizer(stop_words = self.stop_words, \n",
    "                                     ngram_range = self.ngram_range)\n",
    "        counts = count_vect.fit_transform(text_target)\n",
    "        \n",
    "        # TF-IDF\n",
    "        tfidf_transformer = TfidfTransformer(use_idf = self.use_idf)\n",
    "        tfidf = tfidf_transformer.fit_transform(counts)\n",
    "        #tfidf = TfidfVectorizer().fit_transform(text_target)\n",
    "        #cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "        cosine_similarities = (tfidf * tfidf.T).A\n",
    "        #squareform(pdist(tfidf.toarray(), 'cosine'))\n",
    "        return cosine_similarities[:-len(self.target),len(text):]\n",
    "    \n",
    "class MyModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "class NumberTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        num_text_col = text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))[0-9][0-9.,\\-\\/]*(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "                    value='NUMBERSPECIALTOKEN',inplace=False,regex=True)\n",
    "        return num_text_col\n",
    "\n",
    "# =============================================================================\n",
    "#         text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))[0-9][0-9.,\\-\\/]*(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "#                     value='NUMBERSPECIALTOKEN',inplace=True,regex=True)\n",
    "#         return text_col\n",
    "# =============================================================================\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "# =============================================================================\n",
    "#         text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))(\\d+[/-]\\d+[/-]\\d+)(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "#                 value='DATESPECIALTOKEN',inplace=True,regex=True)\n",
    "# =============================================================================\n",
    "        pattern=self.getDatePattern()\n",
    "        date_text_col = text_col.replace(to_replace=re.compile(pattern,flags = re.IGNORECASE),\n",
    "                 value='DATESPECIALTOKEN',inplace=False,regex=True)\n",
    "        return date_text_col\n",
    "    \n",
    "    def getDatePattern(self):\n",
    "        short_month_names = (\n",
    "            'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "            'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
    "        )\n",
    "        \n",
    "        long_month_names = (\n",
    "            'January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
    "            'August', 'September', 'October', 'November', 'December'\n",
    "        )\n",
    "        \n",
    "        short_month_cap = '(?:' + '|'.join(short_month_names) + ')'\n",
    "        long_month_cap = '(?:' + '|'.join(long_month_names) + ')'\n",
    "        short_num_month_cap = '(?:[1-9]|1[12])'\n",
    "        long_num_month_cap = '(?:0[1-9]|1[12])'\n",
    "        \n",
    "        long_day_cap = '(?:0[1-9]|[12][0-9]|3[01])'\n",
    "        short_day_cap = '(?:[1-9]|[12][0-9]|3[01])'\n",
    "        \n",
    "        long_year_cap = '(?:[0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]|[0-9][1-9][0-9]{2}|[1-9][0-9]{3})'\n",
    "        short_year_cap = '(?:[0-9][0-9])'\n",
    "        \n",
    "        ordinal_day = '(?:2?1st|2?2nd|2?3rd|[12]?[4-9]th|1[123]th|[123]0th|31st)'\n",
    "        spl_char='\\W{1}'\n",
    "        \n",
    "        formats = (\n",
    "            r'(?P<month_0>{lnm}|{snm}){sp_c}(?P<day_0>{ld}|{sd}){sp_c}(?P<year_0>{ly}|{sy})',\n",
    "            r'(?P<month_1>{sm})\\-(?P<day_1>{ld}|{sd})\\-(?P<year_1>{ly})',\n",
    "            r'(?P<month_2>{sm}|{lm})(?:\\.\\s+|\\s*)(?P<day_2>{ld}|{sd})(?:,\\s+|\\s*)(?P<year_2>{ly})',\n",
    "            r'(?P<day_3>{ld}|{sd})(?:[\\.,]\\s+|\\s*)(?P<month_3>{lm}|{sm})(?:[\\.,]\\s+|\\s*)(?P<year_3>{ly})',\n",
    "            r'(?P<month_4>{lm}|{sm})\\s+(?P<year_4>{ly})',\n",
    "            r'(?P<month_5>{lnm}|{snm})/(?P<year_5>{ly})',\n",
    "            r'(?P<year_6>{ly})',\n",
    "            r'(?P<month_6>{sm})\\s+(?P<day_4>(?={od})[0-9][0-9]?)..,\\s*(?P<year_7>{ly})'\n",
    "        )\n",
    "        \n",
    "        _pattern = '|'.join(\n",
    "            i.format(\n",
    "                sm=short_month_cap, lm=long_month_cap, snm=short_num_month_cap,sp_c=spl_char,\n",
    "                lnm=long_num_month_cap, ld=long_day_cap, sd=short_day_cap,\n",
    "                ly=long_year_cap, sy=short_year_cap, od=ordinal_day\n",
    "            ) for i in formats\n",
    "        )\n",
    "        return _pattern\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "   \n",
    "class SynonymTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def __init__(self, synonym_dict):\n",
    "        self.syn_dict = synonym_dict\n",
    "        #print(self.syn_dict)\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        #print(self.syn_dict)\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        #print(self.syn_dict)\n",
    "        date_text_col = text_col.replace(self.syn_dict,regex=True)\n",
    "        return date_text_col\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "\n",
    "class PunctTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        regexp = '['+string.punctuation+']{2,}'\n",
    "        punct_text_col = text_col.replace(to_replace=re.compile(regexp,\n",
    "                                                                flags = re.IGNORECASE),\n",
    "                                             value='',inplace=False,regex=True)\n",
    "        return punct_text_col\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "\n",
    "    \n",
    "class FeaturizeDomainKeyWords(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def __init__(self, domain_keyword_list = []):\n",
    "        self.keywords_list = domain_keyword_list\n",
    "        #print(self.keywords_list)\n",
    "    \n",
    "    def fit(self, x, y = None):\n",
    "        #print(self.keywords_list)\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        #print(self.keywords_list)\n",
    "        keyword_textcol_list = []\n",
    "        if self.keywords_list != None:\n",
    "            for text in text_col:\n",
    "                keyword_textcol_dict = {}\n",
    "                for keywords in self.keywords_list:\n",
    "                    if(len(keywords) > 1):\n",
    "                        keyword_reg = \"|\".join(keywords)\n",
    "                    else:\n",
    "                        keyword_reg = keywords[0]\n",
    "                    keyword = 'has_keyword_' + keywords[0]\n",
    "                    keyword_textcol_dict[keyword] = bool(re.search(keyword_reg,text,re.IGNORECASE))\n",
    "                keyword_textcol_list.append(keyword_textcol_dict)\n",
    "        return keyword_textcol_list\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "        keyword_col_list = ['has_keyword_' + keywords[0] for keywords in self.keywords_list]\n",
    "        return keyword_col_list\n",
    "    \n",
    "def ClassDiscriminatingMeasure(X,y):\n",
    "    CDM_tk =np.zeros(shape=(X.shape[1],))\n",
    "#full_term_sum = tr_dsc_vect.tocsr().sum(0)\n",
    "    for category in np.unique(y):\n",
    "        #print(category)\n",
    "        pos_loc = np.where(y == category)[0]\n",
    "        cat_num_doc = len(pos_loc)\n",
    "        #print(cat_num_doc)\n",
    "        neg_loc = np.where(y != category)[0]\n",
    "        neg_cat_num_doc = len(neg_loc)\n",
    "        #print(neg_cat_num_doc)\n",
    "        cat_term = X.tocsr()[pos_loc,:]\n",
    "        #(nonzero_rows,nonzero_cols,_)=sparse.find(cat_term)\n",
    "        tk_ci = np.diff(cat_term.tocsc().indptr)\n",
    "        P_tk_ci = tk_ci / cat_num_doc\n",
    "        #cat_term_sum = cat_term.sum(0)\n",
    "        cat_term_neg = X.tocsr()[neg_loc,:]\n",
    "        #cat_term_neg_sum = cat_term_neg.sum(0)\n",
    "        #(nonzero_rows,nonzero_cols)=cat_term_neg.nonzero()\n",
    "        tk_neg_ci = np.diff(cat_term_neg.tocsc().indptr)\n",
    "        P_tk_neg_ci = (1 + tk_neg_ci)/ neg_cat_num_doc\n",
    "        CDM_tk_ci = np.log1p(P_tk_ci/P_tk_neg_ci)\n",
    "        CDM_tk = CDM_tk + CDM_tk_ci\n",
    "    #print(CDM_tk.shape)\n",
    "    return CDM_tk\n",
    "\n",
    "def get_context_d_tk_w(d, tk, w = 3,token_regex = r\"(?u)\\b\\w\\w+\\b\"):\n",
    "    #sentence = sentence.split()\n",
    "    #d = re.split('[\\s\\-\\:]+',d)\n",
    "    r_splt = re.compile(token_regex)\n",
    "    d = r_splt.findall(d)\n",
    "    len_d = len(d)\n",
    "    tk = tk.split()\n",
    "    num_words = len(tk)\n",
    "    r_st = re.compile(r\"\\b%s\\b\" % tk[0], re.IGNORECASE|re.MULTILINE)\n",
    "    r_cmp = re.compile(r\"\\b%s\\b\" % ' '.join(tk), re.IGNORECASE|re.MULTILINE)\n",
    "    for i,word in enumerate(d):\n",
    "        if bool(r_st.match(word)) and \\\n",
    "        bool(r_cmp.match(' '.join(d[i:i+num_words]))):\n",
    "            #print(i)\n",
    "            #print(word)\n",
    "            begin_pad = []\n",
    "            end_pad = []\n",
    "            if (i-w < 0):\n",
    "                for b in reversed(range(0,w-i)):\n",
    "                    begin_pad.append('__START_'+ str(b) +'__')\n",
    "            #print(begin_pad)\n",
    "            if (i+num_words+w > len_d):\n",
    "                for e in range(0,i+num_words+w - len_d):\n",
    "                    end_pad.append('__END_'+ str(e) +'__')\n",
    "            #print(end_pad)\n",
    "            start = max(0, i-w)\n",
    "            #print(d[start:i+num_words+w])\n",
    "            begin_pad.extend(d[start:i+num_words+w])\n",
    "            #print(begin_pad)\n",
    "            begin_pad.extend(end_pad)\n",
    "            yield ' '.join(begin_pad)\n",
    "\n",
    "def pairs(*lists):\n",
    "    for t in itertools.combinations(lists, 2):\n",
    "        for pair in itertools.product(*t):\n",
    "            yield pair\n",
    "            \n",
    "def get_sim_context_d_tk_w(docs, tk, m_w = 3):\n",
    "    sim_context_all_w = []\n",
    "    for w in reversed(range(0,m_w + 1)):\n",
    "        doc_contexts = []\n",
    "        doc_contexts_itr = docs.apply(get_context_d_tk_w,args = (tk,w))\n",
    "        doc_context_num = []\n",
    "        for context in doc_contexts_itr:\n",
    "            list_context = list(context)\n",
    "            doc_context_num.append(len(list_context)) \n",
    "            doc_contexts.append(list_context)\n",
    "        sim_context_w = []\n",
    "        for x in pairs(*doc_contexts):\n",
    "            sim_context_w.append(distance.get_jaro_distance(x[0],x[1]))\n",
    "        sim_context_all_w.append(sim_context_w)\n",
    "    sim_context_all_w = np.asarray(sim_context_all_w)\n",
    "    sim_context_all_w = sim_context_all_w.sum(0)/ (m_w + 1)\n",
    "    #range_list = []\n",
    "    sim_context_d_tk_w = []\n",
    "    for i in range(len(doc_context_num)):\n",
    "        cr = 0\n",
    "        range_list = []\n",
    "        for pair in itertools.combinations(list(range(len(doc_context_num))),2): \n",
    "            #print(pair)\n",
    "            last_pos = cr + (doc_context_num[pair[0]] * doc_context_num[pair[1]])\n",
    "            all_pos = list(range(cr,last_pos))\n",
    "            #print(all_pos)\n",
    "            cr = last_pos      \n",
    "            if i in pair:\n",
    "                range_list.extend(all_pos)\n",
    "        #range_list.append(tmp_range_list)\n",
    "        sim_context_d_tk_w.append(sum(sim_context_all_w[range_list]))\n",
    "    return(sim_context_d_tk_w)\n",
    "       \n",
    "def ClassDiscriminatingMeasureCS(X,y):\n",
    "    CDM_tk =np.zeros(shape=(X.shape[1],))\n",
    "#full_term_sum = tr_dsc_vect.tocsr().sum(0)\n",
    "    for category in np.unique(y):\n",
    "        #print(category)\n",
    "        pos_loc = np.where(y == category)[0]\n",
    "        cat_num_doc = len(pos_loc)\n",
    "        #print(cat_num_doc)\n",
    "        neg_loc = np.where(y != category)[0]\n",
    "        neg_cat_num_doc = len(neg_loc)\n",
    "        #print(neg_cat_num_doc)\n",
    "        cat_term = X.tocsr()[pos_loc,:]\n",
    "        #(nonzero_rows,nonzero_cols,_)=sparse.find(cat_term)\n",
    "        tk_ci = cat_term.sum(0)\n",
    "        P_tk_ci = tk_ci / cat_num_doc\n",
    "        #cat_term_sum = cat_term.sum(0)\n",
    "        cat_term_neg = X.tocsr()[neg_loc,:]\n",
    "        #cat_term_neg_sum = cat_term_neg.sum(0)\n",
    "        #(nonzero_rows,nonzero_cols)=cat_term_neg.nonzero()\n",
    "        tk_neg_ci = cat_term_neg.sum(0)\n",
    "        P_tk_neg_ci = (1 + tk_neg_ci)/ neg_cat_num_doc\n",
    "        CDM_tk_ci = np.log1p(P_tk_ci/P_tk_neg_ci)\n",
    "        CDM_tk = CDM_tk + CDM_tk_ci\n",
    "    #print((CDM_tk.A1).shape)\n",
    "    return  CDM_tk.A1\n",
    "\n",
    "def get_sim_context_tk_w(terms,\n",
    "                       count_vect_obj,\n",
    "                       raw_document,\n",
    "                       max_window = 3,\n",
    "                       token_regex = r\"(?u)\\b\\w\\w+\\b\",\n",
    "                       stop_words = None,\n",
    "                       cache_dir = None):\n",
    "                                           #'(?u)\\\\b\\\\w\\\\w+\\\\b'):\n",
    "    cache_dict = {}\n",
    "    is_cache = False\n",
    "    cache_update = False\n",
    "    if (cache_dir != None and os.path.isdir(cache_dir)):\n",
    "        is_cache = True\n",
    "        file_sign_str = (raw_document.str.cat(sep = ' ') + str(max_window)).encode(encoding = 'utf-8')\n",
    "        hash_object = hashlib.md5(file_sign_str)\n",
    "        cache_file_path = cache_dir + '/' + hash_object.hexdigest() + '.pkl'\n",
    "        if os.path.isfile(cache_file_path):\n",
    "            with open (cache_file_path, 'rb') as fp:\n",
    "                cache_dict = pickle.load(fp)\n",
    "    term_list = count_vect_obj.get_feature_names()\n",
    "    raw_document.index = range(len(raw_document))\n",
    "    #r_splt = re.compile(\"%s\" % token_regex)\n",
    "    data_lower = raw_document.str.lower().str.findall(token_regex)\n",
    "    if stop_words != None:\n",
    "        data_lower_stop = data_lower.apply(lambda x: ' '.join([item for item in x if item not in stop_words]))\n",
    "    else:\n",
    "        data_lower_stop = data_lower\n",
    "    nz_rows, nz_cols, nz_val = find(terms) #.nonzero()\n",
    "    num_terms = terms.shape[1]\n",
    "    ret_mat = lil_matrix(terms.shape)\n",
    "    for term_idx in range(0,(num_terms-1)):\n",
    "        #print(term_idx)\n",
    "        term_doc_indx = nz_rows[np.where(nz_cols == term_idx)[0]]\n",
    "        #nz_val[np.where(nz_cols == term_idx)[0]]\n",
    "        if (len(term_doc_indx) == 1):\n",
    "            ret_mat[term_doc_indx,term_idx] = 0 # should this be 1 instead as unique term\n",
    "        else:\n",
    "           tk =  term_list[term_idx]\n",
    "           docs = data_lower_stop[term_doc_indx]\n",
    "           if len(cache_dict) > 0 and tk in cache_dict:\n",
    "               sim_context_d_tk_w = cache_dict[tk]\n",
    "           else:    \n",
    "               sim_context_d_tk_w = get_sim_context_d_tk_w(docs,tk,max_window)\n",
    "               if is_cache:\n",
    "                   cache_dict[tk] = sim_context_d_tk_w\n",
    "                   cache_update = True\n",
    "           for i,row_idx in enumerate(term_doc_indx):\n",
    "               ret_mat[row_idx,term_idx] = sim_context_d_tk_w[i]\n",
    "    \n",
    "    if is_cache and cache_update:\n",
    "        lock = filelock.FileLock(\"{}.lock\".format(cache_file_path))\n",
    "        try:\n",
    "            with lock.acquire(timeout = 10):\n",
    "                with open(cache_file_path, 'wb') as fp:\n",
    "                    pickle.dump(cache_dict, fp)\n",
    "        except lock.Timeout:\n",
    "            print('update_cache timeout' + cache_file_path)\n",
    "    #CDM_tk = ClassDiscriminatingMeasure(ret_mat,y,'sum')\n",
    "    return ret_mat\n",
    "\n",
    "\n",
    "class ContextSimilarityBasedFeatureSelection(CountVectorizer):\n",
    "    def __init__(self,max_window = 3,\n",
    "                 input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None,\n",
    "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), analyzer='word',\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 vocabulary=None, binary=False, dtype=np.int64,\n",
    "                 percentile = 10,cache_dir = None):\n",
    "        super(ContextSimilarityBasedFeatureSelection, self).__init__(input,\n",
    "             encoding, decode_error, strip_accents, lowercase , preprocessor,\n",
    "             tokenizer, stop_words, token_pattern, ngram_range ,\n",
    "             analyzer, max_df, min_df, max_features, vocabulary, binary, \n",
    "             dtype)\n",
    "        self.max_window = max_window\n",
    "        self.token_pattern = token_pattern\n",
    "        self.stop_words = stop_words\n",
    "        #self.percentile = percentile\n",
    "        self._red_dim = SelectPercentile(score_func=ClassDiscriminatingMeasureCS,\n",
    "                                         percentile = percentile)\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "    @property\n",
    "    def percentile(self):\n",
    "        return self._red_dim.percentile\n",
    "\n",
    "    @percentile.setter\n",
    "    def percentile(self, value):\n",
    "        self._red_dim.percentile = value\n",
    "    \n",
    "    @property\n",
    "    def score_func(self):\n",
    "        return self._red_dim.score_func\n",
    "\n",
    "    @score_func.setter\n",
    "    def score_func(self, value):\n",
    "        self._red_dim.score_func = value\n",
    "# =============================================================================\n",
    "#     def fit(self, raw_documents, y=None):\n",
    "#         return self\n",
    "# =============================================================================\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        dtm = super(ContextSimilarityBasedFeatureSelection, self).fit_transform(raw_documents)\n",
    "        sim_context_tk_w = get_sim_context_tk_w(terms = dtm,\n",
    "                       count_vect_obj = super(ContextSimilarityBasedFeatureSelection, self),\n",
    "                       raw_document = raw_documents,\n",
    "                       max_window = self.max_window,\n",
    "                       token_regex = self.token_pattern,\n",
    "                       stop_words = self.stop_words,\n",
    "                       cache_dir = self.cache_dir)\n",
    "        self._red_dim.fit_transform(sim_context_tk_w,y)\n",
    "        self.selected_cols = self._red_dim.get_support(indices=True)\n",
    "        return dtm[:,self.selected_cols]\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        #check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')\n",
    "        dtm = super(ContextSimilarityBasedFeatureSelection, self).transform(raw_documents)\n",
    "        return dtm[:,self.selected_cols]\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        all_features = super(ContextSimilarityBasedFeatureSelection, self).get_feature_names()\n",
    "        return np.asarray(all_features)[self.selected_cols]\n",
    "\n",
    "def classifaction_report_df(report):\n",
    "    report = re.sub(r\" +\", \" \", report).replace(\"avg / total\", \"avg/total\").replace(\"\\n \", \"\\n\")\n",
    "    report_df = pd.read_csv(StringIO(\"Classes\" + report), sep=' ', index_col=0)        \n",
    "    return(report_df)\n",
    "# =============================================================================\n",
    "#     report_data = []\n",
    "#     lines = report.split('\\n')\n",
    "#     for line in lines[2:-3]:\n",
    "#         row = {}\n",
    "#         row_data = line.split('      ')\n",
    "#         row['class'] = row_data[0]\n",
    "#         row['precision'] = float(row_data[1])\n",
    "#         row['recall'] = float(row_data[2])\n",
    "#         row['f1_score'] = float(row_data[3])\n",
    "#         row['support'] = float(row_data[4])\n",
    "#         report_data.append(row)\n",
    "#     dataframe = pd.DataFrame.from_dict(report_data)\n",
    "# #    dataframe.to_csv('classification_report.csv', index = False)\n",
    "#     return dataframe\n",
    "# =============================================================================\n",
    "\n",
    "def get_used_features(mod,explicit_feature_selection = True):\n",
    "    if explicit_feature_selection:\n",
    "        mod_support = mod.named_steps['feature_selection'].get_support(indices=True)\n",
    "    features = []\n",
    "    for trnf_list in mod.named_steps['union'].transformer_list:\n",
    "        features.extend(trnf_list[1].named_steps['vect'].get_feature_names())\n",
    "    if explicit_feature_selection:\n",
    "        return(np.asarray(features)[mod_support])\n",
    "    else:\n",
    "        return(np.asarray(features))\n",
    "\n",
    "def get_grid_values(gs_obj):\n",
    "    means = gs_obj.cv_results_['mean_test_score']\n",
    "    stds = gs_obj.cv_results_['std_test_score']\n",
    "    col_name = ['means', 'stds']\n",
    "    col_name.extend(list(gs_obj.cv_results_['params'][0].keys()))\n",
    "    perf_df = pd.DataFrame(columns=col_name )    \n",
    "    i = 0   \n",
    "    for mean, std, params in zip(means, stds, gs_obj.cv_results_['params']):\n",
    "# =============================================================================\n",
    "#         print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#                   % (mean, std * 2, params))\n",
    "# =============================================================================\n",
    "        row_list = [mean,std * 2]   \n",
    "        row_list.extend(params.values())\n",
    "        perf_df.loc[i] = row_list\n",
    "        i += 1\n",
    "    return perf_df\n",
    "\n",
    "def confusion_matrix_df(y_actu,y_pred):\n",
    "    y_actu = pd.Series(y_actu, name='Actual')\n",
    "    y_pred = pd.Series(y_pred, name='Predicted')\n",
    "    #return np.array2string(confusion_matrix, separator=', ')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred, \n",
    "                               rownames=['Actual'], colnames=['Predicted'], \n",
    "                               margins=True)\n",
    "    return df_confusion\n",
    "\n",
    "\n",
    "class BodyExtraction(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, body_extract='Pick_Full_Mail',set_num_words=100):\n",
    "        #pick_First_Mail\n",
    "        #Pick_Words\n",
    "        #Pick_Full_Mail\n",
    "        self.body_extract = body_extract\n",
    "        self.set_num_words=set_num_words\n",
    "        #print(self.keywords_list)\n",
    "    \n",
    "    def fit(self, x, y = None):\n",
    "        #print(self.keywords_list)\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        #print(self.keywords_list)\n",
    "        \n",
    "        \n",
    "        text_col = text_col.map(lambda x:self.clean(x))\n",
    "        \n",
    "        \n",
    "        #sentence extraction or replacing to,cc \n",
    "        text_col = text_col.replace(to_replace=re.compile(r'To:.*?(?=SEPARATOR)|Cc:.*?(?=SEPARATOR)|Subject:.*?(?=SEPARATOR)|Sent:.*?(?=SEPARATOR)',flags = re.IGNORECASE),\n",
    "                    value='',inplace=False,regex=True)\n",
    "        \n",
    "        \n",
    "        # removing disclaimers\n",
    "        text_col = text_col.replace(to_replace=re.compile(r'this email and the document[\\w\\W]+this message is intended solely for the use[\\w\\W]+|//na01.safelinks.protection.outlook.com[\\w\\W]+|note:? this is a system generated email[\\w\\W]+|this email transmission, and any documents, files or previous email messages[\\w\\W]+|this email  including attachments  is confidential[\\w\\W]+|this email  including attachments  is confidential[\\w\\W]+|notice: this email and any attachments are for the exclusive and confidential[\\w\\W]+|the information in this email is confidential[\\w\\W]+|this message and its attachments are intended for the exclusive[\\w\\W]+|this email and any files transmitted with it are confidential and intended[\\w\\W]+|kind este mensaje[\\w\\W]+|the contents of this email message and any attachments are intended solely[\\w\\W]+|this message and its attachments are the property of aerovías de méxico[\\w\\W]+|this is an automated response.for any assistance [\\w\\W]+|confidentiality notice this communication may contain privileged or confidential information[\\w\\W]+|the contents of this email and any attachments may contain confidential information[\\w\\W]+|this email message and any attachments are for the use of the intended recipients[\\w\\W]+|as informações contidas nesta mensagem são confidenciais[\\w\\W ]+',flags = re.IGNORECASE),\n",
    "                    value='',inplace=False,regex=True)\n",
    "        \n",
    "        text_col=text_col.map(lambda x:self.remove_Non_Word(x))\n",
    "        \n",
    "        text_col=text_col.map(lambda x:self.remove_Repeated_Words(x))\n",
    "    \n",
    "        ##############remove salutations#################\n",
    "        sal_regex=self.salutations_regex()\n",
    "        text_col = text_col.replace(to_replace=re.compile(sal_regex,flags = re.IGNORECASE),\n",
    "                 value='',inplace=False,regex=True)\n",
    "        \n",
    "        if(self.body_extract=='pick_First_Mail'):\n",
    "            \n",
    "            text_col=text_col.map(lambda x: ' '.join(x.split('THREADCHAINBREAK')[0:1]))\n",
    "            \n",
    "            transformed_col=text_col.replace(to_replace=re.compile(r'\\s+',flags = re.IGNORECASE),\n",
    "                    value=' ',inplace=False,regex=True)\n",
    "            \n",
    "            return transformed_col\n",
    "            \n",
    "        elif(self.body_extract=='Pick_Words'):\n",
    "            \n",
    "            transformed_col = text_col.replace(to_replace='THREADCHAINBREAK',\n",
    "                 value=' ',inplace=False,regex=True)\n",
    "            transformed_col=transformed_col.replace(to_replace=re.compile(r'\\s+',flags = re.IGNORECASE),\n",
    "                    value=' ',inplace=False,regex=True)\n",
    "            \n",
    "            return transformed_col.map(lambda x:' '.join(x.split(' ')[0:self.set_num_words]))\n",
    "        \n",
    "        elif(self.body_extract=='Pick_Full_Mail'):\n",
    "            \n",
    "            transformed_col=text_col.replace(to_replace='THREADCHAINBREAK',value=' ',inplace=False,regex=True)\n",
    "            return transformed_col.replace(to_replace=re.compile(r'\\s+',flags = re.IGNORECASE),\n",
    "                    value=' ',inplace=False,regex=True)\n",
    "        \n",
    "    def clean(self,text):\n",
    "        text = text.replace('\\r' , '')\n",
    "        text = text.replace('\\t' , '')\n",
    "        text = text.replace('?','').replace('-','')\n",
    "        text = text.replace('\\xa0',' ')\n",
    "        text = text.replace('\\n','SEPARATOR')\n",
    "        #text = re.sub('\\W' ,' ', text)\n",
    "        text =  re.sub('\\w+@\\w+.domain','email_set', text)\n",
    "        text = re.sub('<http://[\\w//.]+>' ,' url_set', text)\n",
    "        text = re.sub('http://[\\w.]+.com' ,'', text)\n",
    "        text = re.sub('<html[\\w/\\\"<>=., ]+' ,'', text)\n",
    "        text = re.sub('Disclaimer:[a-zA-Z0-9_,(). ]+' ,'', text)\n",
    "        text = re.sub('www.[\\w]+.com' ,'', text)\n",
    "    #        text = re.sub('Date:' ,'Sent:', text)\n",
    "        text = re.sub('CC:' ,'Cc:', text)\n",
    "        text = re.sub('Expéditeur:' ,'From:', text)\n",
    "        text = re.sub('De:' ,'From:', text)\n",
    "        text = re.sub('Enviado el:' ,'Sent:', text)\n",
    "        text = re.sub('Asunto:' ,'Subject:', text)\n",
    "        text = re.sub('Enviada em:' ,'Sent:', text)\n",
    "        text = re.sub('Assunto:' ,'Subject:', text)\n",
    "        \n",
    "        text = re.sub(' url_set' ,'', text)\n",
    "        text = text + ' From:'\n",
    "        text=re.sub(r'From:.*?(?=SEPARATOR)','THREADCHAINBREAK',text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_Non_Word(self,docs):\n",
    "        docs = re.sub(r'cid:email_set','',docs)\n",
    "        docs = re.sub(r'mailto:email_set','',docs)\n",
    "        docs = re.sub(r'email_set','',docs)\n",
    "        docs = re.sub(r'description:','',docs)\n",
    "        docs = re.sub(r'original message','',docs)\n",
    "        docs = re.sub(r'[\\*\\[\\]_<>/#|!=\\-\\&;\\?^\"\"]','',docs)\n",
    "        docs = re.sub(r'\\s+', ' ', docs)\n",
    "        if docs.startswith('clientname internal'):\n",
    "            docs = docs.replace('clientname internal' , '')\n",
    "        docs = re.sub(r\"\\'\", '', docs)\n",
    "        a = re.search(r'good (morning|afternoon)' , docs)\n",
    "        if a is not None: \n",
    "            docs = docs.replace(a.group(),'')\n",
    "        if docs.startswith('dear'):\n",
    "            docs = re.sub(r'dear','', docs)\n",
    "        docs = re.sub(r'(hi,|hello|hi )','',docs)\n",
    "        #docs = re.sub(r',',' ',docs)\n",
    "        docs = docs.strip()\n",
    "        return docs\n",
    "    \n",
    "    def remove_Repeated_Words(self,docs):\n",
    "        docs = re.sub('importance:|cc:|re:|:','',docs)\n",
    "        docs = re.sub('\\.+','.',docs)\n",
    "        docs = re.sub('SEPARATOR' , ' ',docs)\n",
    "        docs = re.sub('\\s+',' ',docs)\n",
    "        docs = docs.strip()\n",
    "        docs=docs.strip(' From')\n",
    "        return docs\n",
    "    \n",
    "    def salutations_regex(self):\n",
    "        \n",
    "        salutations=['Mit freundlichen Gruben','Thank you for your assistance','Thanks and Regards','Many thanks in advance','Thanks a lot',\n",
    "         'Thanks for the support','Thanks for your help','Thanks for all','Thank you for your support','Thanks in advance','Thanks so much','Thanks again',\n",
    "         'Thank you','Thanks you','Thanks & Regards','Many thanks','Best','Thank you in advance for your help',\n",
    "         'Thx','TKS','Thanks','Kind regards','Best regards','Cordialement','Saludos',\n",
    "         'Disclaimer','Sincerely','Brgds','With Regards','Krgds','Regards','Have a Wonderful Day',\n",
    "         'Please do not respond to this message','Visit the exciting new options','Rgds',\n",
    "         'Caution','This e-mail message and any attachment(s)','Please hurry']\n",
    "        \n",
    "        return '|'.join(salutations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "##Predefined packages needed\n",
    "import argparse\n",
    "import configparser\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support,classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "##from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "#from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import make_scorer,f1_score\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, SelectFromModel, f_classif\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_name = 'MYORG'\n",
    "file_encoding = 'utf8'\n",
    "test_size_param =0.2\n",
    "val_size_param = 0.2\n",
    "body_extract_param = 'Pick_Full_Mail'\n",
    "##choices will be (‘pick_First_Mail’,’Pick_Words’,’ Pick_Full_Mail’)\n",
    "   \n",
    "##Predefined Model Hyper Parameters to be part of config file\n",
    "#N_FEATURES_OPTIONS = [2, 4, 8]\n",
    "N_FEATURES_chi = [30, 20, 10]\n",
    "##C_OPTIONS = [1, 10, 100, 1000]\n",
    "NUMBER_OF_ESTIMATORS_RF = [ 80]\n",
    "stknb_alpha_param = [0.01]\n",
    "ngram_range_param_subj = [(1, 2)]\n",
    "ngram_range_param_desc = [(1, 2)]\n",
    "alpha_param = [1e-2]\n",
    "true_false_param = [True, False]\n",
    "##log_C = (1,10)\n",
    "log_C = [1]\n",
    "##max_df_param = (0.25, 0.5, 0.75)\n",
    "max_df_param = [0.75]\n",
    "min_df_param = [0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'D:/Data Science/POC/Email Classification Product/debug'\n",
    "synonym_file_path = 'D:/Data Science/POC/Email Classification Product/csv/synonyms.csv'\n",
    "domain_keyword_file_path = 'D:/Data Science/POC/Email Classification Product/csv/domain_key_words.csv'\n",
    "stop_words_file = 'D:/Data Science/POC/Email Classification Product/csv/stop_words_not_masked.csv'\n",
    "from sklearn.feature_extraction import text\n",
    "if (stop_words_file != ''):\n",
    "        my_additional_stop_words = re.split('\\n|\\t', open(stop_words_file).read())\n",
    "        my_stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "else:\n",
    "        my_stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA INGESTION COMPLETED\n",
      "Shipment Status                               425\n",
      "For your information cases                    279\n",
      "Expedite request                               68\n",
      "Internal Team Request to order status team     40\n",
      "Quote status                                   28\n",
      "Name: Text.Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_file_path = \"D:/Data Science/POC/Email Classification Product/Email_Final_Input_Desc_Dedup.csv\"\n",
    "data_df = pd.read_csv(data_file_path,encoding=file_encoding) \n",
    "print(\"DATA INGESTION COMPLETED\")\n",
    "data_df.dropna(inplace=True)\n",
    "target_col = 'Text.Category'\n",
    "##training_subject_flag = 'Y'\n",
    "if 'Text.Subj' in data_df.columns.values:\n",
    "            training_col = ['Text.Body','Text.Subj']\n",
    "else: \n",
    "            training_col = ['Text.Body']\n",
    "\n",
    "print(data_df[target_col].value_counts())\n",
    "        ###Split the  Data into Training /Test and Validation for usage\n",
    "training_val, test = train_test_split(data_df,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify=data_df[target_col])\n",
    "training, validation = train_test_split(training_val,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify=training_val[target_col])\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "        ###SAVE THE FILES TO O/P DIRECTORY\n",
    "training.to_csv(output_dir + '/' + portfolio_name + '_training_' + now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                                   index  = False, )\n",
    "validation.to_csv(output_dir + '/' + portfolio_name + '_validation_' + now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                                   index  = False, )\n",
    "test.to_csv(output_dir + '/' + portfolio_name + '_test_' + now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                                   index  = False, )\n",
    "X_cat = training[training_col]\n",
    "Y_cat = training[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE FIT COMPLETED\n",
      "{'memory': None, 'steps': [('union', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('content_desc', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), (...rue,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]))],\n",
      "       transformer_weights=None)), ('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "        norm_order=1, prefit=False, threshold=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))], 'union': FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('content_desc', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), (...rue,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]))],\n",
      "       transformer_weights=None), 'feature_selection': SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "        norm_order=1, prefit=False, threshold=None), 'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'union__n_jobs': 1, 'union__transformer_list': [('content_desc', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', T...\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None))])), ('content_stats', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]))], 'union__transformer_weights': None, 'union__content_desc': Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', T...\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None))]), 'union__content_stats': Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]), 'union__content_desc__memory': None, 'union__content_desc__steps': [('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=frozenset({'', 'even', 'must', 'detail', 'urlhttp', 'yours', 'if', 'in', 'and ', 'between', 'email', 'into', 'across', 'forty', 'three', 'httpsaerospaceclientnamecom', 'never', 'july', 'co', 'becomes', 'moreover', 'under', 'would', 'aerohttp', 'namely', 'thin', 'whereby', 'seem', 'call', ...clientnam', 'although', 'afterwards', 'six', 'became', 'when', 'why', 'formerly', 'very', 'ponopo'}),\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None))], 'union__content_desc__selector': ItemSelector(key='Text.Body'), 'union__content_desc__preprocessing': BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100), 'union__content_desc__datetrns': DateTransformer(), 'union__content_desc__numtrns': NumberTransformer(), 'union__content_desc__puntrns': PunctTransformer(), 'union__content_desc__synonyms': SynonymTransformer(synonym_dict=None), 'union__content_desc__vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=frozenset({'', 'even', 'must', 'detail', 'urlhttp', 'yours', 'if', 'in', 'and ', 'between', 'email', 'into', 'across', 'forty', 'three', 'httpsaerospaceclientnamecom', 'never', 'july', 'co', 'becomes', 'moreover', 'under', 'would', 'aerohttp', 'namely', 'thin', 'whereby', 'seem', 'call', ...clientnam', 'although', 'afterwards', 'six', 'became', 'when', 'why', 'formerly', 'very', 'ponopo'}),\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'union__content_desc__selector__key': 'Text.Body', 'union__content_desc__preprocessing__body_extract': 'Pick_Full_Mail', 'union__content_desc__preprocessing__set_num_words': 100, 'union__content_desc__synonyms__synonym_dict': None, 'union__content_desc__vect__analyzer': 'word', 'union__content_desc__vect__binary': False, 'union__content_desc__vect__decode_error': 'strict', 'union__content_desc__vect__dtype': <class 'numpy.int64'>, 'union__content_desc__vect__encoding': 'utf-8', 'union__content_desc__vect__input': 'content', 'union__content_desc__vect__lowercase': True, 'union__content_desc__vect__max_df': 1.0, 'union__content_desc__vect__max_features': None, 'union__content_desc__vect__min_df': 1, 'union__content_desc__vect__ngram_range': (1, 1), 'union__content_desc__vect__norm': 'l2', 'union__content_desc__vect__preprocessor': None, 'union__content_desc__vect__smooth_idf': True, 'union__content_desc__vect__stop_words': frozenset({'', 'even', 'must', 'detail', 'urlhttp', 'yours', 'if', 'in', 'and ', 'between', 'email', 'into', 'across', 'forty', 'three', 'httpsaerospaceclientnamecom', 'never', 'july', 'co', 'becomes', 'moreover', 'under', 'would', 'aerohttp', 'namely', 'thin', 'whereby', 'seem', 'call', 'ltd', 'cant', 'almost', 'please', 'therein', 'mostly', 'an', 'ref ', 'whither', 'am', 'whereas', 'https', 'thankschip', 'sometimes', 'front', 'enough', 'show', 'for', 'last', 'well', 'interest', 'none', 'from', 'hereupon', 'hundred', 'perhaps', 'as', 'latterly', 'mailtosomeemailaddressdomain', 'thursday', 'whole', 'do', 'five', 'again', 'go', 'being', 'indeed', 'some', 'two', 'still', 'anyhow', 'this', 'cidsomeemailaddressdomain', 'give', 'nowhere', 'inc', 'meanwhile', 'bill', 'amount', 'except', 'which', 'wherever', 'thence', 'however', 'side', 'alone', 'casepr ', 'thru', 'someemailaddressdomain', 'during', 'was', 'june', 'nevertheless', 'them', 'has', 'further', 'everything', 'elsewhere', 'could', 'because', 'while', 'can', 'the', 'at', 'my', 'where', 'after', 'full', 'wednesday', 'may', 'over', 'both', 'twenty', 'eg', 'empty', 'something', 'move', 'her', 'are', 'now', 'on', 'such', 'what', 'get', 'seems', 'via', 'whom', 'onto', 'http', 'who', 'ie', 'nor', 'down', 'that', 'each', 'around', 'about', 'how', 'system', 'among', 'found', 'to', 'rather', 'had', 'i', 'take', 'urlhttpaffwww', 'ever', 'along', 'through', 'behind', 'hereafter', 'whereupon', 'plea', 'will', 'nobody', 'hers', 'more', 'sixty', 'a', 'she', 'thus', 'couldnt', 'hence', 'somehow', 'own', 'might', 'by', 'else', 'etc', 'herein', 'quotationpr ', 'name', 'yourself', 'their', 'clientnameaerone', 'clientnamenowne', 'former', 'nothing', 'ourselves', 'de', 'mobilepr ', 'tuesday', 'someemailaddress', 'next', 'they', 'also', 'you', 'nine', 'ponopono', 'always', 'is', 'twelve', 'us', 'describe', 'fire', 'then', 'four', 'mill', 'otherwise', 'mailto', 'everyone', 'its', 'above', 'no', 'thereafter', 'here', 'only', 'out', 'monday', 'against', 'anything', 'due', 'since', 'somewhere', 'already', 'beforehand', 'sunday', 'latter', 'be', 'someone', 'these', 'yet', 'pleas', 'made', 'becoming', 'myself', 'often', 'un', 'much', 'saturday', 'fill', 'amoungst', 'domain', 'your', 'seemed', 'throughout', 'least', 'either', 'part', 'so', 'than', 'whose', 'up', 'were', 'hereby', 'ten', 'mail', 'pono', 'seeming', 'wclientnam', 'without', 'of', 'cry', 'sincere', 'towards', 'comhttp', 'him', 'his', 'though', 'besides', 'herself', 'back', 'whoever', 'put', 'within', 'with', 'thereby', 'off', 'total ', 'most', 'first', 'have', 'eight', 'everywhere', 'itself', 'another', 'many', 'order ', 'done', 'whereafter', 'someemailaddressclientname', 'anyway', 'clientname', 'bottom', 'our', 'whenever', 'ours', 'every', 'thereupon', 'any', 'all', 'me', 'once', 'eleven', 'those', 'himself', 'less', 'and', 'we', 'same', 'ushttp', 'several', 'sometime', 'fifteen', 'jan', 'clientnameaerohttp', 'become', 'www', 'been', 'find', 'not', 'per', 'he', 'beyond', 'noone', 'until', 'friday', 'anyone', 'clientnamecom', 'anywhere', 'but', 'or', 'whence', 'august', 'others', 'one', 'amongst', 'clientnam', 'upon', 'neither', 'wherein', 'beside', 'thick', 'therefore', 'cannot', 'see', 'whether', 'hasnt', 'other', 'html', 'toward', 'req ', 'should', 'before', 'tosomeemailaddressdomain', 'keep', 'too', 'yourselves', 'mine', 'together', 'fifty', 'below', 'wwwclientnam', 'third', 'whatever', 'top', 'con', 'few', 'serious', 'themselves', 'it', 're', 'clientnamenow', 'httpsaerospaceclientnamecomencontactus', 'there', 'comfdataponocordersclientnam', 'although', 'afterwards', 'six', 'became', 'when', 'why', 'formerly', 'very', 'ponopo'}), 'union__content_desc__vect__strip_accents': None, 'union__content_desc__vect__sublinear_tf': False, 'union__content_desc__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'union__content_desc__vect__tokenizer': None, 'union__content_desc__vect__use_idf': True, 'union__content_desc__vect__vocabulary': None, 'union__content_stats__memory': None, 'union__content_stats__steps': [('selector', ItemSelector(key='Text.Body')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))], 'union__content_stats__selector': ItemSelector(key='Text.Body'), 'union__content_stats__stats': TextStats(), 'union__content_stats__vect': DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True), 'union__content_stats__scale': StandardScaler(copy=True, with_mean=False, with_std=True), 'union__content_stats__selector__key': 'Text.Body', 'union__content_stats__vect__dtype': <class 'numpy.float64'>, 'union__content_stats__vect__separator': '=', 'union__content_stats__vect__sort': True, 'union__content_stats__vect__sparse': True, 'union__content_stats__scale__copy': True, 'union__content_stats__scale__with_mean': False, 'union__content_stats__scale__with_std': True, 'feature_selection__estimator__C': 1.0, 'feature_selection__estimator__class_weight': None, 'feature_selection__estimator__dual': False, 'feature_selection__estimator__fit_intercept': True, 'feature_selection__estimator__intercept_scaling': 1, 'feature_selection__estimator__loss': 'squared_hinge', 'feature_selection__estimator__max_iter': 1000, 'feature_selection__estimator__multi_class': 'ovr', 'feature_selection__estimator__penalty': 'l1', 'feature_selection__estimator__random_state': None, 'feature_selection__estimator__tol': 0.0001, 'feature_selection__estimator__verbose': 0, 'feature_selection__estimator': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0), 'feature_selection__norm_order': 1, 'feature_selection__prefit': False, 'feature_selection__threshold': None, 'clf__alpha': 1.0, 'clf__class_prior': None, 'clf__fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "pipeline_featureunion_list_desc = []\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer \n",
    "    ##Part of config file\n",
    "    \n",
    "    \n",
    "trim_words_param = 100\n",
    "    \n",
    "if synonym_file_path!= '':    \n",
    "        with open(synonym_file_path, mode='r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            syn_dict = {rows[0]:rows[1] for rows in reader}\n",
    "        pipeline_featureunion_list_desc.append(('content_desc', Pipeline([\n",
    "                                        ('selector', ItemSelector(key='Text.Body')),\n",
    "                                        ('preprocessing', BodyExtraction(body_extract=body_extract_param,set_num_words=trim_words_param)),\n",
    "                                        ('datetrns', DateTransformer()),\n",
    "                                        ('numtrns', NumberTransformer()),\n",
    "                                        ('puntrns', PunctTransformer()),\n",
    "                                        ('synonyms',SynonymTransformer(syn_dict)),\n",
    "#                                        ('vect', CountVectorizer(stop_words = my_stop_words)),\n",
    "                                        ('vect', TfidfVectorizer(stop_words = my_stop_words)),\n",
    "#                                        ('tfidf', TfidfTransformer()),\n",
    "                                ])))\n",
    "else:\n",
    "        pipeline_featureunion_list_desc.append(('content_desc', Pipeline([\n",
    "                                            ('selector', ItemSelector(key='Text.Body')),\n",
    "                                            ('preprocessing', BodyExtraction(body_extract=body_extract_param,set_num_words=trim_words_param)),\n",
    "                                            ('datetrns', DateTransformer()),\n",
    "                                            ('numtrns', NumberTransformer()),\n",
    "                                            ('puntrns', PunctTransformer()),\n",
    "#                                            ('vect', CountVectorizer(stop_words = my_stop_words)),\n",
    "                                            ('vect', TfidfVectorizer(stop_words = my_stop_words)),\n",
    "#                                            ('tfidf', TfidfTransformer()),\n",
    "                                    ]))) \n",
    "pipeline_featureunion_list_desc.append(('content_stats', Pipeline([\n",
    "        ('selector', ItemSelector(key='Text.Body')),                            \n",
    "        ('stats', TextStats()),  # returns a list of dicts\n",
    "                                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                                    ('scale',StandardScaler(with_mean=False)),\n",
    "                                ])))\n",
    "    \n",
    "     ###CREATE SUBJECT PIPELINE\n",
    "if 'Text.Subj' in data_df.columns.values:\n",
    "        if synonym_file_path!= '':    \n",
    "            with open(synonym_file_path, mode='r') as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                syn_dict = {rows[0]:rows[1] for rows in reader}\n",
    "            pipeline_featureunion_list_desc.append(('content_subj', Pipeline([\n",
    "                                            ('selector', ItemSelector(key='Text.Subj')),\n",
    "                                            ('preprocessing', BodyExtraction(body_extract=body_extract_param,set_num_words=trim_words_param)),\n",
    "                                            ('datetrns', DateTransformer()),\n",
    "                                            ('numtrns', NumberTransformer()),\n",
    "                                            ('puntrns', PunctTransformer()),\n",
    "                                            ('synonyms',SynonymTransformer(syn_dict)),\n",
    "                                            ('vect', CountVectorizer(stop_words = my_stop_words)),\n",
    "#                                            ('tfidf', TfidfTransformer()),\n",
    "                                    ])))\n",
    "        else:\n",
    "            pipeline_featureunion_list_desc.append(('content_subj', Pipeline([\n",
    "                                                ('selector', ItemSelector(key='Text.Subj')),\n",
    "                                                ('preprocessing', BodyExtraction(body_extract=body_extract_param,set_num_words=trim_words_param)),\n",
    "                                                ('datetrns', DateTransformer()),\n",
    "                                                ('numtrns', NumberTransformer()),\n",
    "                                                ('puntrns', PunctTransformer()),\n",
    "                                                ('vect', CountVectorizer(stop_words = my_stop_words)),\n",
    "#                                                ('tfidf', TfidfTransformer()),\n",
    "                                        ]))) \n",
    "        pipeline_featureunion_list_desc.append(('content_sub_stats', Pipeline([\n",
    "            ('selector', ItemSelector(key='Text.Subj')),                            \n",
    "            ('stats', TextStats()),  # returns a list of dicts\n",
    "                                        ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                                        ('scale',StandardScaler(with_mean=False)),\n",
    "                                    ])))\n",
    "        \n",
    "        \n",
    "pipeline_features_mnb = Pipeline([('union', FeatureUnion(\n",
    "                            transformer_list=pipeline_featureunion_list_desc,\n",
    "                            )),\n",
    "                             #('scale',StandardScaler(with_mean=False)),\n",
    "                             ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\",dual=False))),\n",
    "                             ('clf', MultinomialNB())])\n",
    "    \n",
    "text_clf_mnb = pipeline_features_mnb.fit(X_cat, Y_cat)\n",
    "    \n",
    "print(\"PIPELINE FIT COMPLETED\")\n",
    "print(pipeline_features_mnb.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537, 1)\n",
      "(537,)\n",
      "                                             Text.Body\n",
      "415  ______________________________________________...\n",
      "317  Pham - we had agreed to honor the $2761 price ...\n",
      "255  Hi Sir/Madam, \\r\\r\\n\\r\\r\\nWe placed a new PO L...\n",
      "777  Sure Allen, let me purchase and will get back ...\n",
      "659                                   IDOC#XXXXXXXXXX \n",
      "['Case.Number' 'Text.Category' 'Text.Body']\n"
     ]
    }
   ],
   "source": [
    "print(X_cat.shape)\n",
    "print(Y_cat.shape)\n",
    "print(X_cat.head())\n",
    "print(data_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP2\n"
     ]
    }
   ],
   "source": [
    "if 'Text.Subj' in data_df.columns.values:\n",
    "    parameters_nb = [{\n",
    "                                'union__content_desc__vect':  [CountVectorizer(stop_words = my_stop_words),\n",
    "                                             StemmedCountVectorizer(stop_words = my_stop_words)],\n",
    "                                'union__content_desc__vect__ngram_range': ngram_range_param_desc,\n",
    "                                'union__content_desc__vect__max_df': max_df_param,\n",
    "                                'union__content_desc__vect__min_df': min_df_param,\n",
    "#                                'union__content_desc__tfidf__use_idf': true_false_param,\n",
    "                                'union__content_subj__vect':  [CountVectorizer(stop_words = my_stop_words),\n",
    "                                             StemmedCountVectorizer(stop_words = my_stop_words)],\n",
    "#                                'union__content_subj__vect__ngram_range': ngram_range_param_subj,\n",
    "#                                'union__content_subj__vect__max_df': max_df_param,\n",
    "#                                'union__content_subj__vect__min_df': min_df_param,\n",
    "#                                'union__content_subj__tfidf__use_idf': true_false_param,\n",
    "                                'clf__alpha': alpha_param\n",
    "                            },\n",
    "                            {\n",
    "                                'feature_selection': [SelectPercentile(chi2)],\n",
    "                                'feature_selection__percentile': N_FEATURES_chi\n",
    "                            },\n",
    "                            {\n",
    "                                'feature_selection': [SelectFromModel(ExtraTreesClassifier())],\n",
    "                                'feature_selection__estimator__n_estimators' : NUMBER_OF_ESTIMATORS_RF\n",
    "    \n",
    "                            },\n",
    "                            {   'feature_selection': [SelectPercentile(f_classif)],\n",
    "                                'feature_selection__percentile': N_FEATURES_chi\n",
    "    \n",
    "                            },\n",
    "                            {\n",
    "                                'feature_selection': [SelectFromModel(LinearSVC(penalty=\"l1\",dual=False, C =0.01))],\n",
    "                            }]\n",
    "    \n",
    "else:\n",
    "    parameters_nb = [{\n",
    "                                'union__content_desc__vect':  [CountVectorizer(stop_words = my_stop_words),\n",
    "                                             StemmedCountVectorizer(stop_words = my_stop_words)],\n",
    "                                'union__content_desc__vect__ngram_range': ngram_range_param_desc,\n",
    "                                'union__content_desc__vect__max_df': max_df_param,\n",
    "                                'union__content_desc__vect__min_df': min_df_param,\n",
    "#                                'union__content_desc__tfidf__use_idf': true_false_param,\n",
    "                                'clf__alpha': alpha_param\n",
    "                            },\n",
    "                            {\n",
    "                                'feature_selection': [SelectPercentile(chi2)],\n",
    "                                'feature_selection__percentile': N_FEATURES_chi\n",
    "                            },\n",
    "                            {\n",
    "                                'feature_selection': [SelectFromModel(ExtraTreesClassifier())],\n",
    "                                'feature_selection__estimator__n_estimators' : NUMBER_OF_ESTIMATORS_RF\n",
    "    \n",
    "                            },\n",
    "                            {   'feature_selection': [SelectPercentile(f_classif)],\n",
    "                                'feature_selection__percentile': N_FEATURES_chi\n",
    "    \n",
    "                            },\n",
    "                            {\n",
    "                                'feature_selection': [SelectFromModel(LinearSVC(penalty=\"l1\",dual=False, C =0.01))],\n",
    "                            }]\n",
    "    \n",
    "    \n",
    "        \n",
    "    ###instance of the grid search by passing the classifier, parameters \n",
    "    # and n_jobs= -1 which tells to use multiple cores from user machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'union__content_desc__vect': [CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=frozenset({'', 'even', 'must', 'detail', 'urlhttp', 'yours', 'if', 'in', 'and ', 'between', 'email', 'into', 'across', 'forty', 'three', 'httpsaerospaceclientnamecom', 'never', 'july', 'co', 'becomes', 'moreover', 'under', 'would', 'aerohttp', 'namely', 'thin', 'whereby', 'seem', 'call', ...clientnam', 'although', 'afterwards', 'six', 'became', 'when', 'why', 'formerly', 'very', 'ponopo'}),\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor=None,\n",
      "            stop_words=frozenset({'', 'even', 'must', 'detail', 'urlhttp', 'yours', 'if', 'in', 'and ', 'between', 'email', 'into', 'across', 'forty', 'three', 'httpsaerospaceclientnamecom', 'never', 'july', 'co', 'becomes', 'moreover', 'under', 'would', 'aerohttp', 'namely', 'thin', 'whereby', 'seem', 'call', ...clientnam', 'although', 'afterwards', 'six', 'became', 'when', 'why', 'formerly', 'very', 'ponopo'}),\n",
      "            strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "            tokenizer=None, vocabulary=None)], 'union__content_desc__vect__ngram_range': [(1, 2)], 'union__content_desc__vect__max_df': [0.75], 'union__content_desc__vect__min_df': [0.01], 'clf__alpha': [0.01]}, {'feature_selection': [SelectPercentile(percentile=10,\n",
      "         score_func=<function chi2 at 0x00000159246F7620>)], 'feature_selection__percentile': [30, 20, 10]}, {'feature_selection': [SelectFromModel(estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
      "        norm_order=1, prefit=False, threshold=None)], 'feature_selection__estimator__n_estimators': [80]}, {'feature_selection': [SelectPercentile(percentile=10,\n",
      "         score_func=<function f_classif at 0x00000159246F7510>)], 'feature_selection__percentile': [30, 20, 10]}, {'feature_selection': [SelectFromModel(estimator=LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "        norm_order=1, prefit=False, threshold=None)]}]\n"
     ]
    }
   ],
   "source": [
    "print(parameters_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRID SEARCH FIT completed for Naive Bayes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshuman_mahapatra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "f_scorer = make_scorer(f1_score, average = 'weighted')\n",
    "gs_clf_mnb = GridSearchCV(text_clf_mnb, parameters_nb, scoring = f_scorer,\n",
    "                              cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=32))\n",
    "    \n",
    "    \n",
    "gs_clf_mnb = gs_clf_mnb.fit(X_cat, Y_cat)\n",
    "    \n",
    "print(\"GRID SEARCH FIT completed for Naive Bayes\")\n",
    "    ##Multinomial Naive Bayes estimators\n",
    "mod1 = gs_clf_mnb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics = pd.DataFrame()\n",
    "Metrics['Models'] = ['NaiveBayes', 'XGB', 'StackedModel']\n",
    "Metirics['Models']\n",
    "\n",
    "import pandas as pd\n",
    "data = [{'Model_type': NaiveBayes, 'Model_Validation_Accuracy': 2, 'Model_Validation_Precision':3, 'Model_Validation_Recall':4},\n",
    "        {'Model_type': XGB, 'Model_Validation_Accuracy': 2, 'Model_Validation_Precision':3, 'Model_Validation_Recall':4}\n",
    "        {'Model_type': StackedNBXGBoost, 'Model_Validation_Accuracy': 2, 'Model_Validation_Precision':3, 'Model_Validation_Recall':4}]\n",
    "= pd.DataFrame(data)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='weighted'))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-1] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
