{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "     Case.Number                    Category  \\\n",
      "0  CASE-14820434             Shipment Status   \n",
      "1  CASE-14820436  For your information cases   \n",
      "2  CASE-14820436  For your information cases   \n",
      "3  CASE-99999999  For your information cases   \n",
      "\n",
      "                                           Text.Body        Date  \n",
      "0  Hi Order Team,\\r\\n\\r\\nPls check payment and re...  11/19/2018  \n",
      "1  No it is not, it is another p/n Antenna-TCAS D...  10/19/2018  \n",
      "2  No it is not, it is another p/n Antenna-TCAS D...  11/19/2018  \n",
      "3  ABC XYZ Disclaimer:This email and any files tr...  11/19/2018  \n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.feature_selection import SelectFromModel, f_classif\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "print(__doc__)\n",
    "\n",
    "###File Ingestion\n",
    "training_file = 'D:/Data Science/POC/Email Classification Product/Email_Dump_Testfile.csv'\n",
    "encoding_format = 'cp1252'\n",
    "id_column = \"Case.Number\"\n",
    "text_column = \"Text.Body\"\n",
    "date_column = \"Date\"\n",
    "disclaimer_data = \"\"\n",
    "initial_file  = pd.read_csv(training_file,encoding= encoding_format)\n",
    "print(type(initial_file))\n",
    "\n",
    "\n",
    "###Sort and dedup function:-\n",
    "def sort_dedup(initial_file):\n",
    "    if id_column in initial_file.columns.values:\n",
    "        if date_column in initial_file.columns.values:\n",
    "            input_file_sort = initial_file.sort_values([id_column, date_column], ascending=[False, False])\n",
    "            training_input = input_file_sort.drop_duplicates(subset = id_column, keep ='first', inplace=False)\n",
    "            return(training_input)\n",
    "        else:\n",
    "            training = initial_file.drop_duplicates(subset = id_column,keep ='first', inplace=False)\n",
    "            return(training_input)\n",
    "        \n",
    "    else: \n",
    "        print(\"Sort Dedup not required as we dont have duplicates\")\n",
    "        return(initial_file)\n",
    "    \n",
    "\n",
    "\n",
    "print(initial_file.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Case.Number                    Category  \\\n",
      "3  CASE-99999999  For your information cases   \n",
      "2  CASE-14820436  For your information cases   \n",
      "0  CASE-14820434             Shipment Status   \n",
      "\n",
      "                                           Text.Body        Date  \n",
      "3  ABC XYZ Disclaimer:This email and any files tr...  11/19/2018  \n",
      "2  No it is not, it is another p/n Antenna-TCAS D...  11/19/2018  \n",
      "0  Hi Order Team,\\r\\n\\r\\nPls check payment and re...  11/19/2018  \n"
     ]
    }
   ],
   "source": [
    "training = sort_dedup(initial_file)\n",
    "print(training.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Case.Number                    Category  \\\n",
      "3  CASE-99999999  For your information cases   \n",
      "2  CASE-14820436  For your information cases   \n",
      "0  CASE-14820434             Shipment Status   \n",
      "\n",
      "                                           Text.Body        Date  \n",
      "3  ABC XYZ DISCLAIMER email and any files transmi...  11/19/2018  \n",
      "2  No it is not, it is another p/n Antenna-TCAS D...  11/19/2018  \n",
      "0  Hi Order Team,\\r\\n\\r\\nPls check payment and re...  11/19/2018  \n"
     ]
    }
   ],
   "source": [
    "## remove all disclaimers from emails based on the data provided in config file\n",
    "\n",
    "##training['Text.Body'].replace(disclaimer_data, \"DISCLAIMER\", inplace=True)\n",
    "training['Text.Body'].replace(to_replace=r'Disclaimer:\\s?[A-Z]+.',value = 'DISCLAIMER', regex= True,inplace=True)\n",
    "print(training.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "var = \"1\"\n",
    "my_pipe =[]\n",
    "my_pipe.append(('contents'+var,Pipeline([\n",
    "    ('feature_selection', SelectFromModel(ExtraTreesClassifier())),\n",
    "    ('classify', LinearSVC())\n",
    "])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "##my_pipe.append(('pipe1', Pipeline([('tfidf', TfidfVectorizer( stop_words='english'))])))\n",
    "my_pipe.append(('contents',Pipeline([('tfidf', TfidfVectorizer( stop_words='english'))])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f6c768806903>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read config file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfigparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfigParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mini_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mportfolio_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DEFAULT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'portfolio_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DEFAULT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_col'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(args.ini_file)\n",
    "portfolio_name = config['DEFAULT']['portfolio_name']\n",
    "target_col = config['DEFAULT']['target_col']\n",
    "desc_col = config['DEFAULT']['desc_col']\n",
    "id_col = config['DEFAULT']['id_col']\n",
    "file_encoding = config['DEFAULT']['file_encoding']\n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "stop_words_file = config['DEFAULT']['stop_words_file']\n",
    "domain_keyword_file_path = config['DEFAULT']['domain_keyword_file_path']\n",
    "synonym_file_path = config['DEFAULT']['synonym_file_path']\n",
    "training_subject_flag = config['DEFAULT']['synonym_file_path']\n",
    "training_cols = config['DEFAULT']['training_cols']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predefined packages needed\n",
    "import argparse\n",
    "import configparser\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support,classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "#from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import make_scorer,f1_score\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, SelectFromModel, f_classif\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "f_scorer = make_scorer(f1_score, average = 'weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "##file paths\n",
    "synonym_file_path = 'D:/Data Science/POC/Email Classification Product/csv/synonyms.csv'\n",
    "domain_keyword_file_path = 'D:/Data Science/POC/Email Classification Product/csv/domain_key_words.csv'\n",
    "stop_words_file = 'D:/Data Science/POC/Email Classification Product/csv/stop_words_not_masked.csv'\n",
    "from sklearn.feature_extraction import text\n",
    "if (stop_words_file != ''):\n",
    "    my_additional_stop_words = re.split('\\n|\\t', open(stop_words_file).read())\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "else:\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Data Science/POC/Email Classification Product/csv/synonyms.csv\n"
     ]
    }
   ],
   "source": [
    "print(synonym_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipment Status                               424\n",
      "For your information cases                    279\n",
      "Expedite request                               68\n",
      "Internal Team Request to order status team     40\n",
      "Quote status                                   28\n",
      "Name: Text.Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "####Training File Ingestion\n",
    "##Need the data to be having 'Text.Body', 'Text.Category' and  'Text.Subj'(optional field)\n",
    "##data_file_path = config[args.run_mode.upper()]['data_file_path']\n",
    "###file_encoding = config['DEFAULT']['file_encoding']\n",
    "###training_col = ['DEFAULT']['training_col']\n",
    "portfolio_name = 'MYORG'\n",
    "file_encoding = 'utf8'\n",
    "data_file_path = 'D:/Data Science/POC/Email Classification Product/Email_Final_Input_SubjDesc_Dedup.csv'\n",
    "output_dir = 'D:/Data Science/POC/Email Classification Product/'\n",
    "data_df = pd.read_csv(data_file_path,encoding=file_encoding) \n",
    "data_df.dropna(inplace=True)\n",
    "target_col = 'Text.Category'\n",
    "##training_subject_flag = 'Y'\n",
    "if 'Text.Subj' in data_df.columns.values:\n",
    "    training_col = ['Text.Body','Text.Subj']\n",
    "else: \n",
    "    training_col = ['Text.Body']\n",
    "    \n",
    "print(data_df[target_col].value_counts())\n",
    "###Split the  Data into Training /Test and Validation for usage\n",
    "training_val, test = train_test_split(data_df,\n",
    "                                            test_size = 0.2,\n",
    "                                            stratify=data_df[target_col])\n",
    "training, validation = train_test_split(training_val,\n",
    "                                            test_size = 0.2,\n",
    "                                            stratify=training_val[target_col])\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "###SAVE THE FILES TO O/P DIRECTORY\n",
    "training.to_csv(output_dir + '/' + portfolio_name + '_training_' + now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                           index  = False, )\n",
    "validation.to_csv(output_dir + '/' + portfolio_name + '_validation_' + now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                           index  = False, )\n",
    "test.to_csv(output_dir + '/' + portfolio_name + '_test_' + now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                           index  = False, )\n",
    "X_cat = training[training_col]\n",
    "Y_cat = training[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Text.Body  \\\n",
      "534  Hello,\\r\\r\\n\\r\\r\\nEnd customer L-3 requires th...   \n",
      "192  Hi client_name Team,\\r\\r\\n\\r\\r\\nPlease find ou...   \n",
      "829  client_name Team,\\r\\r\\n\\r\\r\\nPlease provide in...   \n",
      "479  OrderXXXXXXXXXX /VF \\r\\r\\nPN :XXXXXXXXXX ,CAPA...   \n",
      "36   Hello,\\r\\r\\n\\r\\r\\nCan you please verify the le...   \n",
      "\n",
      "                                             Text.Subj  \n",
      "534  RE: Toronto Clearing House - ZRIT to Clearwate...  \n",
      "192  RE: Weekly Status Request (V:SCY35_OPEN2017/06...  \n",
      "829  RE: Invoice Request // PO# 8R0530    [ ref:_00...  \n",
      "479  RE: ORDER ON HOLD - PO# 3050024596/VF -  PN : ...  \n",
      "36   RE: P/N 2-300-196-01 Lead Time    [ ref:_00D30...  \n"
     ]
    }
   ],
   "source": [
    "print(X_cat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534    For your information cases\n",
      "192               Shipment Status\n",
      "829               Shipment Status\n",
      "479               Shipment Status\n",
      "36                Shipment Status\n",
      "Name: Text.Category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Y_cat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predefined functions:-Util class\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#import csv\n",
    "import string\n",
    "from scipy.sparse import lil_matrix, find\n",
    "import itertools\n",
    "#from pyjarowinkler import distance\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "import os.path\n",
    "import hashlib\n",
    "import pickle\n",
    "import filelock\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_df):\n",
    "        return data_df[self.key]\n",
    "    \n",
    "    def get_feature_names():\n",
    "       return []\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': (text.count('.') + \\\n",
    "                                   text.count('?') + \\\n",
    "                                   text.count('!'))}\n",
    "                for text in posts]\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return ['length','num_sentences']\n",
    "\n",
    "class TargetSimilarity(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, target,stop_words = None, ngram_range = (1,3),use_idf = False):\n",
    "        self.target = target\n",
    "        self.stop_words = stop_words\n",
    "        self.ngram_range = ngram_range\n",
    "        self.use_idf = use_idf\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text):\n",
    "        text_target = np.append(text,self.target)\n",
    "        count_vect = StemmedCountVectorizer(stop_words = self.stop_words, \n",
    "                                     ngram_range = self.ngram_range)\n",
    "        counts = count_vect.fit_transform(text_target)\n",
    "        \n",
    "        # TF-IDF\n",
    "        tfidf_transformer = TfidfTransformer(use_idf = self.use_idf)\n",
    "        tfidf = tfidf_transformer.fit_transform(counts)\n",
    "        #tfidf = TfidfVectorizer().fit_transform(text_target)\n",
    "        #cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "        cosine_similarities = (tfidf * tfidf.T).A\n",
    "        #squareform(pdist(tfidf.toarray(), 'cosine'))\n",
    "        return cosine_similarities[:-len(self.target),len(text):]\n",
    "    \n",
    "class MyModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "class NumberTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        num_text_col = text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))[0-9][0-9.,\\-\\/]*(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "                    value='NUMBERSPECIALTOKEN',inplace=False,regex=True)\n",
    "        return num_text_col\n",
    "\n",
    "# =============================================================================\n",
    "#         text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))[0-9][0-9.,\\-\\/]*(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "#                     value='NUMBERSPECIALTOKEN',inplace=True,regex=True)\n",
    "#         return text_col\n",
    "# =============================================================================\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "# =============================================================================\n",
    "#         text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))(\\d+[/-]\\d+[/-]\\d+)(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "#                 value='DATESPECIALTOKEN',inplace=True,regex=True)\n",
    "# =============================================================================\n",
    "        pattern=self.getDatePattern()\n",
    "        date_text_col = text_col.replace(to_replace=re.compile(pattern,flags = re.IGNORECASE),\n",
    "                 value='DATESPECIALTOKEN',inplace=False,regex=True)\n",
    "        return date_text_col\n",
    "    \n",
    "    def getDatePattern(self):\n",
    "        short_month_names = (\n",
    "            'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "            'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
    "        )\n",
    "        \n",
    "        long_month_names = (\n",
    "            'January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
    "            'August', 'September', 'October', 'November', 'December'\n",
    "        )\n",
    "        \n",
    "        short_month_cap = '(?:' + '|'.join(short_month_names) + ')'\n",
    "        long_month_cap = '(?:' + '|'.join(long_month_names) + ')'\n",
    "        short_num_month_cap = '(?:[1-9]|1[12])'\n",
    "        long_num_month_cap = '(?:0[1-9]|1[12])'\n",
    "        \n",
    "        long_day_cap = '(?:0[1-9]|[12][0-9]|3[01])'\n",
    "        short_day_cap = '(?:[1-9]|[12][0-9]|3[01])'\n",
    "        \n",
    "        long_year_cap = '(?:[0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]|[0-9][1-9][0-9]{2}|[1-9][0-9]{3})'\n",
    "        short_year_cap = '(?:[0-9][0-9])'\n",
    "        \n",
    "        ordinal_day = '(?:2?1st|2?2nd|2?3rd|[12]?[4-9]th|1[123]th|[123]0th|31st)'\n",
    "        spl_char='\\W{1}'\n",
    "        \n",
    "        formats = (\n",
    "            r'(?P<month_0>{lnm}|{snm}){sp_c}(?P<day_0>{ld}|{sd}){sp_c}(?P<year_0>{ly}|{sy})',\n",
    "            r'(?P<month_1>{sm})\\-(?P<day_1>{ld}|{sd})\\-(?P<year_1>{ly})',\n",
    "            r'(?P<month_2>{sm}|{lm})(?:\\.\\s+|\\s*)(?P<day_2>{ld}|{sd})(?:,\\s+|\\s*)(?P<year_2>{ly})',\n",
    "            r'(?P<day_3>{ld}|{sd})(?:[\\.,]\\s+|\\s*)(?P<month_3>{lm}|{sm})(?:[\\.,]\\s+|\\s*)(?P<year_3>{ly})',\n",
    "            r'(?P<month_4>{lm}|{sm})\\s+(?P<year_4>{ly})',\n",
    "            r'(?P<month_5>{lnm}|{snm})/(?P<year_5>{ly})',\n",
    "            r'(?P<year_6>{ly})',\n",
    "            r'(?P<month_6>{sm})\\s+(?P<day_4>(?={od})[0-9][0-9]?)..,\\s*(?P<year_7>{ly})'\n",
    "        )\n",
    "        \n",
    "        _pattern = '|'.join(\n",
    "            i.format(\n",
    "                sm=short_month_cap, lm=long_month_cap, snm=short_num_month_cap,sp_c=spl_char,\n",
    "                lnm=long_num_month_cap, ld=long_day_cap, sd=short_day_cap,\n",
    "                ly=long_year_cap, sy=short_year_cap, od=ordinal_day\n",
    "            ) for i in formats\n",
    "        )\n",
    "        return _pattern\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "   \n",
    "class SynonymTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def __init__(self, synonym_dict):\n",
    "        self.syn_dict = synonym_dict\n",
    "        #print(self.syn_dict)\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        #print(self.syn_dict)\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        #print(self.syn_dict)\n",
    "        date_text_col = text_col.replace(self.syn_dict,regex=True)\n",
    "        return date_text_col\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "\n",
    "class PunctTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        regexp = '['+string.punctuation+']{2,}'\n",
    "        punct_text_col = text_col.replace(to_replace=re.compile(regexp,\n",
    "                                                                flags = re.IGNORECASE),\n",
    "                                             value='',inplace=False,regex=True)\n",
    "        return punct_text_col\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "\n",
    "    \n",
    "class FeaturizeDomainKeyWords(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def __init__(self, domain_keyword_list = []):\n",
    "        self.keywords_list = domain_keyword_list\n",
    "        #print(self.keywords_list)\n",
    "    \n",
    "    def fit(self, x, y = None):\n",
    "        #print(self.keywords_list)\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        #print(self.keywords_list)\n",
    "        keyword_textcol_list = []\n",
    "        if self.keywords_list != None:\n",
    "            for text in text_col:\n",
    "                keyword_textcol_dict = {}\n",
    "                for keywords in self.keywords_list:\n",
    "                    if(len(keywords) > 1):\n",
    "                        keyword_reg = \"|\".join(keywords)\n",
    "                    else:\n",
    "                        keyword_reg = keywords[0]\n",
    "                    keyword = 'has_keyword_' + keywords[0]\n",
    "                    keyword_textcol_dict[keyword] = bool(re.search(keyword_reg,text,re.IGNORECASE))\n",
    "                keyword_textcol_list.append(keyword_textcol_dict)\n",
    "        return keyword_textcol_list\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "        keyword_col_list = ['has_keyword_' + keywords[0] for keywords in self.keywords_list]\n",
    "        return keyword_col_list\n",
    "    \n",
    "def ClassDiscriminatingMeasure(X,y):\n",
    "    CDM_tk =np.zeros(shape=(X.shape[1],))\n",
    "#full_term_sum = tr_dsc_vect.tocsr().sum(0)\n",
    "    for category in np.unique(y):\n",
    "        #print(category)\n",
    "        pos_loc = np.where(y == category)[0]\n",
    "        cat_num_doc = len(pos_loc)\n",
    "        #print(cat_num_doc)\n",
    "        neg_loc = np.where(y != category)[0]\n",
    "        neg_cat_num_doc = len(neg_loc)\n",
    "        #print(neg_cat_num_doc)\n",
    "        cat_term = X.tocsr()[pos_loc,:]\n",
    "        #(nonzero_rows,nonzero_cols,_)=sparse.find(cat_term)\n",
    "        tk_ci = np.diff(cat_term.tocsc().indptr)\n",
    "        P_tk_ci = tk_ci / cat_num_doc\n",
    "        #cat_term_sum = cat_term.sum(0)\n",
    "        cat_term_neg = X.tocsr()[neg_loc,:]\n",
    "        #cat_term_neg_sum = cat_term_neg.sum(0)\n",
    "        #(nonzero_rows,nonzero_cols)=cat_term_neg.nonzero()\n",
    "        tk_neg_ci = np.diff(cat_term_neg.tocsc().indptr)\n",
    "        P_tk_neg_ci = (1 + tk_neg_ci)/ neg_cat_num_doc\n",
    "        CDM_tk_ci = np.log1p(P_tk_ci/P_tk_neg_ci)\n",
    "        CDM_tk = CDM_tk + CDM_tk_ci\n",
    "    #print(CDM_tk.shape)\n",
    "    return CDM_tk\n",
    "\n",
    "def get_context_d_tk_w(d, tk, w = 3,token_regex = r\"(?u)\\b\\w\\w+\\b\"):\n",
    "    #sentence = sentence.split()\n",
    "    #d = re.split('[\\s\\-\\:]+',d)\n",
    "    r_splt = re.compile(token_regex)\n",
    "    d = r_splt.findall(d)\n",
    "    len_d = len(d)\n",
    "    tk = tk.split()\n",
    "    num_words = len(tk)\n",
    "    r_st = re.compile(r\"\\b%s\\b\" % tk[0], re.IGNORECASE|re.MULTILINE)\n",
    "    r_cmp = re.compile(r\"\\b%s\\b\" % ' '.join(tk), re.IGNORECASE|re.MULTILINE)\n",
    "    for i,word in enumerate(d):\n",
    "        if bool(r_st.match(word)) and \\\n",
    "        bool(r_cmp.match(' '.join(d[i:i+num_words]))):\n",
    "            #print(i)\n",
    "            #print(word)\n",
    "            begin_pad = []\n",
    "            end_pad = []\n",
    "            if (i-w < 0):\n",
    "                for b in reversed(range(0,w-i)):\n",
    "                    begin_pad.append('__START_'+ str(b) +'__')\n",
    "            #print(begin_pad)\n",
    "            if (i+num_words+w > len_d):\n",
    "                for e in range(0,i+num_words+w - len_d):\n",
    "                    end_pad.append('__END_'+ str(e) +'__')\n",
    "            #print(end_pad)\n",
    "            start = max(0, i-w)\n",
    "            #print(d[start:i+num_words+w])\n",
    "            begin_pad.extend(d[start:i+num_words+w])\n",
    "            #print(begin_pad)\n",
    "            begin_pad.extend(end_pad)\n",
    "            yield ' '.join(begin_pad)\n",
    "\n",
    "def pairs(*lists):\n",
    "    for t in itertools.combinations(lists, 2):\n",
    "        for pair in itertools.product(*t):\n",
    "            yield pair\n",
    "            \n",
    "def get_sim_context_d_tk_w(docs, tk, m_w = 3):\n",
    "    sim_context_all_w = []\n",
    "    for w in reversed(range(0,m_w + 1)):\n",
    "        doc_contexts = []\n",
    "        doc_contexts_itr = docs.apply(get_context_d_tk_w,args = (tk,w))\n",
    "        doc_context_num = []\n",
    "        for context in doc_contexts_itr:\n",
    "            list_context = list(context)\n",
    "            doc_context_num.append(len(list_context)) \n",
    "            doc_contexts.append(list_context)\n",
    "        sim_context_w = []\n",
    "        for x in pairs(*doc_contexts):\n",
    "            sim_context_w.append(distance.get_jaro_distance(x[0],x[1]))\n",
    "        sim_context_all_w.append(sim_context_w)\n",
    "    sim_context_all_w = np.asarray(sim_context_all_w)\n",
    "    sim_context_all_w = sim_context_all_w.sum(0)/ (m_w + 1)\n",
    "    #range_list = []\n",
    "    sim_context_d_tk_w = []\n",
    "    for i in range(len(doc_context_num)):\n",
    "        cr = 0\n",
    "        range_list = []\n",
    "        for pair in itertools.combinations(list(range(len(doc_context_num))),2): \n",
    "            #print(pair)\n",
    "            last_pos = cr + (doc_context_num[pair[0]] * doc_context_num[pair[1]])\n",
    "            all_pos = list(range(cr,last_pos))\n",
    "            #print(all_pos)\n",
    "            cr = last_pos      \n",
    "            if i in pair:\n",
    "                range_list.extend(all_pos)\n",
    "        #range_list.append(tmp_range_list)\n",
    "        sim_context_d_tk_w.append(sum(sim_context_all_w[range_list]))\n",
    "    return(sim_context_d_tk_w)\n",
    "       \n",
    "def ClassDiscriminatingMeasureCS(X,y):\n",
    "    CDM_tk =np.zeros(shape=(X.shape[1],))\n",
    "#full_term_sum = tr_dsc_vect.tocsr().sum(0)\n",
    "    for category in np.unique(y):\n",
    "        #print(category)\n",
    "        pos_loc = np.where(y == category)[0]\n",
    "        cat_num_doc = len(pos_loc)\n",
    "        #print(cat_num_doc)\n",
    "        neg_loc = np.where(y != category)[0]\n",
    "        neg_cat_num_doc = len(neg_loc)\n",
    "        #print(neg_cat_num_doc)\n",
    "        cat_term = X.tocsr()[pos_loc,:]\n",
    "        #(nonzero_rows,nonzero_cols,_)=sparse.find(cat_term)\n",
    "        tk_ci = cat_term.sum(0)\n",
    "        P_tk_ci = tk_ci / cat_num_doc\n",
    "        #cat_term_sum = cat_term.sum(0)\n",
    "        cat_term_neg = X.tocsr()[neg_loc,:]\n",
    "        #cat_term_neg_sum = cat_term_neg.sum(0)\n",
    "        #(nonzero_rows,nonzero_cols)=cat_term_neg.nonzero()\n",
    "        tk_neg_ci = cat_term_neg.sum(0)\n",
    "        P_tk_neg_ci = (1 + tk_neg_ci)/ neg_cat_num_doc\n",
    "        CDM_tk_ci = np.log1p(P_tk_ci/P_tk_neg_ci)\n",
    "        CDM_tk = CDM_tk + CDM_tk_ci\n",
    "    #print((CDM_tk.A1).shape)\n",
    "    return  CDM_tk.A1\n",
    "\n",
    "def get_sim_context_tk_w(terms,\n",
    "                       count_vect_obj,\n",
    "                       raw_document,\n",
    "                       max_window = 3,\n",
    "                       token_regex = r\"(?u)\\b\\w\\w+\\b\",\n",
    "                       stop_words = None,\n",
    "                       cache_dir = None):\n",
    "                                           #'(?u)\\\\b\\\\w\\\\w+\\\\b'):\n",
    "    cache_dict = {}\n",
    "    is_cache = False\n",
    "    cache_update = False\n",
    "    if (cache_dir != None and os.path.isdir(cache_dir)):\n",
    "        is_cache = True\n",
    "        file_sign_str = (raw_document.str.cat(sep = ' ') + str(max_window)).encode(encoding = 'utf-8')\n",
    "        hash_object = hashlib.md5(file_sign_str)\n",
    "        cache_file_path = cache_dir + '/' + hash_object.hexdigest() + '.pkl'\n",
    "        if os.path.isfile(cache_file_path):\n",
    "            with open (cache_file_path, 'rb') as fp:\n",
    "                cache_dict = pickle.load(fp)\n",
    "    term_list = count_vect_obj.get_feature_names()\n",
    "    raw_document.index = range(len(raw_document))\n",
    "    #r_splt = re.compile(\"%s\" % token_regex)\n",
    "    data_lower = raw_document.str.lower().str.findall(token_regex)\n",
    "    if stop_words != None:\n",
    "        data_lower_stop = data_lower.apply(lambda x: ' '.join([item for item in x if item not in stop_words]))\n",
    "    else:\n",
    "        data_lower_stop = data_lower\n",
    "    nz_rows, nz_cols, nz_val = find(terms) #.nonzero()\n",
    "    num_terms = terms.shape[1]\n",
    "    ret_mat = lil_matrix(terms.shape)\n",
    "    for term_idx in range(0,(num_terms-1)):\n",
    "        #print(term_idx)\n",
    "        term_doc_indx = nz_rows[np.where(nz_cols == term_idx)[0]]\n",
    "        #nz_val[np.where(nz_cols == term_idx)[0]]\n",
    "        if (len(term_doc_indx) == 1):\n",
    "            ret_mat[term_doc_indx,term_idx] = 0 # should this be 1 instead as unique term\n",
    "        else:\n",
    "           tk =  term_list[term_idx]\n",
    "           docs = data_lower_stop[term_doc_indx]\n",
    "           if len(cache_dict) > 0 and tk in cache_dict:\n",
    "               sim_context_d_tk_w = cache_dict[tk]\n",
    "           else:    \n",
    "               sim_context_d_tk_w = get_sim_context_d_tk_w(docs,tk,max_window)\n",
    "               if is_cache:\n",
    "                   cache_dict[tk] = sim_context_d_tk_w\n",
    "                   cache_update = True\n",
    "           for i,row_idx in enumerate(term_doc_indx):\n",
    "               ret_mat[row_idx,term_idx] = sim_context_d_tk_w[i]\n",
    "    \n",
    "    if is_cache and cache_update:\n",
    "        lock = filelock.FileLock(\"{}.lock\".format(cache_file_path))\n",
    "        try:\n",
    "            with lock.acquire(timeout = 10):\n",
    "                with open(cache_file_path, 'wb') as fp:\n",
    "                    pickle.dump(cache_dict, fp)\n",
    "        except lock.Timeout:\n",
    "            print('update_cache timeout' + cache_file_path)\n",
    "    #CDM_tk = ClassDiscriminatingMeasure(ret_mat,y,'sum')\n",
    "    return ret_mat\n",
    "\n",
    "\n",
    "class ContextSimilarityBasedFeatureSelection(CountVectorizer):\n",
    "    def __init__(self,max_window = 3,\n",
    "                 input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None,\n",
    "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), analyzer='word',\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 vocabulary=None, binary=False, dtype=np.int64,\n",
    "                 percentile = 10,cache_dir = None):\n",
    "        super(ContextSimilarityBasedFeatureSelection, self).__init__(input,\n",
    "             encoding, decode_error, strip_accents, lowercase , preprocessor,\n",
    "             tokenizer, stop_words, token_pattern, ngram_range ,\n",
    "             analyzer, max_df, min_df, max_features, vocabulary, binary, \n",
    "             dtype)\n",
    "        self.max_window = max_window\n",
    "        self.token_pattern = token_pattern\n",
    "        self.stop_words = stop_words\n",
    "        #self.percentile = percentile\n",
    "        self._red_dim = SelectPercentile(score_func=ClassDiscriminatingMeasureCS,\n",
    "                                         percentile = percentile)\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "    @property\n",
    "    def percentile(self):\n",
    "        return self._red_dim.percentile\n",
    "\n",
    "    @percentile.setter\n",
    "    def percentile(self, value):\n",
    "        self._red_dim.percentile = value\n",
    "    \n",
    "    @property\n",
    "    def score_func(self):\n",
    "        return self._red_dim.score_func\n",
    "\n",
    "    @score_func.setter\n",
    "    def score_func(self, value):\n",
    "        self._red_dim.score_func = value\n",
    "# =============================================================================\n",
    "#     def fit(self, raw_documents, y=None):\n",
    "#         return self\n",
    "# =============================================================================\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        dtm = super(ContextSimilarityBasedFeatureSelection, self).fit_transform(raw_documents)\n",
    "        sim_context_tk_w = get_sim_context_tk_w(terms = dtm,\n",
    "                       count_vect_obj = super(ContextSimilarityBasedFeatureSelection, self),\n",
    "                       raw_document = raw_documents,\n",
    "                       max_window = self.max_window,\n",
    "                       token_regex = self.token_pattern,\n",
    "                       stop_words = self.stop_words,\n",
    "                       cache_dir = self.cache_dir)\n",
    "        self._red_dim.fit_transform(sim_context_tk_w,y)\n",
    "        self.selected_cols = self._red_dim.get_support(indices=True)\n",
    "        return dtm[:,self.selected_cols]\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        #check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')\n",
    "        dtm = super(ContextSimilarityBasedFeatureSelection, self).transform(raw_documents)\n",
    "        return dtm[:,self.selected_cols]\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        all_features = super(ContextSimilarityBasedFeatureSelection, self).get_feature_names()\n",
    "        return np.asarray(all_features)[self.selected_cols]\n",
    "\n",
    "def classifaction_report_df(report):\n",
    "    report = re.sub(r\" +\", \" \", report).replace(\"avg / total\", \"avg/total\").replace(\"\\n \", \"\\n\")\n",
    "    report_df = pd.read_csv(StringIO(\"Classes\" + report), sep=' ', index_col=0)        \n",
    "    return(report_df)\n",
    "# =============================================================================\n",
    "#     report_data = []\n",
    "#     lines = report.split('\\n')\n",
    "#     for line in lines[2:-3]:\n",
    "#         row = {}\n",
    "#         row_data = line.split('      ')\n",
    "#         row['class'] = row_data[0]\n",
    "#         row['precision'] = float(row_data[1])\n",
    "#         row['recall'] = float(row_data[2])\n",
    "#         row['f1_score'] = float(row_data[3])\n",
    "#         row['support'] = float(row_data[4])\n",
    "#         report_data.append(row)\n",
    "#     dataframe = pd.DataFrame.from_dict(report_data)\n",
    "# #    dataframe.to_csv('classification_report.csv', index = False)\n",
    "#     return dataframe\n",
    "# =============================================================================\n",
    "\n",
    "def get_used_features(mod,explicit_feature_selection = True):\n",
    "    if explicit_feature_selection:\n",
    "        mod_support = mod.named_steps['reduce_dim'].get_support(indices=True)\n",
    "    features = []\n",
    "    for trnf_list in mod.named_steps['union'].transformer_list:\n",
    "        features.extend(trnf_list[1].named_steps['vect'].get_feature_names())\n",
    "    if explicit_feature_selection:\n",
    "        return(np.asarray(features)[mod_support])\n",
    "    else:\n",
    "        return(np.asarray(features))\n",
    "\n",
    "def get_grid_values(gs_obj):\n",
    "    means = gs_obj.cv_results_['mean_test_score']\n",
    "    stds = gs_obj.cv_results_['std_test_score']\n",
    "    col_name = ['means', 'stds']\n",
    "    col_name.extend(list(gs_obj.cv_results_['params'][0].keys()))\n",
    "    perf_df = pd.DataFrame(columns=col_name )    \n",
    "    i = 0   \n",
    "    for mean, std, params in zip(means, stds, gs_obj.cv_results_['params']):\n",
    "# =============================================================================\n",
    "#         print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#                   % (mean, std * 2, params))\n",
    "# =============================================================================\n",
    "        row_list = [mean,std * 2]   \n",
    "        row_list.extend(params.values())\n",
    "        perf_df.loc[i] = row_list\n",
    "        i += 1\n",
    "    return perf_df\n",
    "\n",
    "def confusion_matrix_df(y_actu,y_pred):\n",
    "    y_actu = pd.Series(y_actu, name='Actual')\n",
    "    y_pred = pd.Series(y_pred, name='Predicted')\n",
    "    #return np.array2string(confusion_matrix, separator=', ')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred, \n",
    "                               rownames=['Actual'], colnames=['Predicted'], \n",
    "                               margins=True)\n",
    "    return df_confusion\n",
    "\n",
    "\n",
    "class BodyExtraction(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, body_extract='Pick_Full_Mail',set_num_words=100):\n",
    "        #pick_First_Mail\n",
    "        #Pick_Words\n",
    "        #Pick_Full_Mail\n",
    "        self.body_extract = body_extract\n",
    "        self.set_num_words=set_num_words\n",
    "        #print(self.keywords_list)\n",
    "    \n",
    "    def fit(self, x, y = None):\n",
    "        #print(self.keywords_list)\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        #print(self.keywords_list)\n",
    "        \n",
    "        \n",
    "        text_col = text_col.map(lambda x:self.clean(x))\n",
    "        \n",
    "        \n",
    "        #sentence extraction or replacing to,cc \n",
    "        text_col = text_col.replace(to_replace=re.compile(r'To:.*?(?=SEPARATOR)|Cc:.*?(?=SEPARATOR)|Subject:.*?(?=SEPARATOR)|Sent:.*?(?=SEPARATOR)',flags = re.IGNORECASE),\n",
    "                    value='',inplace=False,regex=True)\n",
    "        \n",
    "        \n",
    "        # removing disclaimers\n",
    "        text_col = text_col.replace(to_replace=re.compile(r'this email and the document[\\w\\W]+this message is intended solely for the use[\\w\\W]+|//na01.safelinks.protection.outlook.com[\\w\\W]+|note:? this is a system generated email[\\w\\W]+|this email transmission, and any documents, files or previous email messages[\\w\\W]+|this email  including attachments  is confidential[\\w\\W]+|this email  including attachments  is confidential[\\w\\W]+|notice: this email and any attachments are for the exclusive and confidential[\\w\\W]+|the information in this email is confidential[\\w\\W]+|this message and its attachments are intended for the exclusive[\\w\\W]+|this email and any files transmitted with it are confidential and intended[\\w\\W]+|kind este mensaje[\\w\\W]+|the contents of this email message and any attachments are intended solely[\\w\\W]+|this message and its attachments are the property of aerovías de méxico[\\w\\W]+|this is an automated response.for any assistance [\\w\\W]+|confidentiality notice this communication may contain privileged or confidential information[\\w\\W]+|the contents of this email and any attachments may contain confidential information[\\w\\W]+|this email message and any attachments are for the use of the intended recipients[\\w\\W]+|as informações contidas nesta mensagem são confidenciais[\\w\\W ]+',flags = re.IGNORECASE),\n",
    "                    value='',inplace=False,regex=True)\n",
    "        \n",
    "        text_col=text_col.map(lambda x:self.remove_Non_Word(x))\n",
    "        \n",
    "        text_col=text_col.map(lambda x:self.remove_Repeated_Words(x))\n",
    "    \n",
    "        ##############remove salutations#################\n",
    "        sal_regex=self.salutations_regex()\n",
    "        text_col = text_col.replace(to_replace=re.compile(sal_regex,flags = re.IGNORECASE),\n",
    "                 value='',inplace=False,regex=True)\n",
    "        \n",
    "        if(self.body_extract=='pick_First_Mail'):\n",
    "            \n",
    "            text_col=text_col.map(lambda x: ' '.join(x.split('THREADCHAINBREAK')[0:1]))\n",
    "            \n",
    "            transformed_col=text_col.replace(to_replace=re.compile(r'\\s+',flags = re.IGNORECASE),\n",
    "                    value=' ',inplace=False,regex=True)\n",
    "            \n",
    "            return transformed_col\n",
    "            \n",
    "        elif(self.body_extract=='Pick_Words'):\n",
    "            \n",
    "            transformed_col = text_col.replace(to_replace='THREADCHAINBREAK',\n",
    "                 value=' ',inplace=False,regex=True)\n",
    "            transformed_col=transformed_col.replace(to_replace=re.compile(r'\\s+',flags = re.IGNORECASE),\n",
    "                    value=' ',inplace=False,regex=True)\n",
    "            \n",
    "            return transformed_col.map(lambda x:' '.join(x.split(' ')[0:self.set_num_words]))\n",
    "        \n",
    "        elif(self.body_extract=='Pick_Full_Mail'):\n",
    "            \n",
    "            transformed_col=text_col.replace(to_replace='THREADCHAINBREAK',value=' ',inplace=False,regex=True)\n",
    "            return transformed_col.replace(to_replace=re.compile(r'\\s+',flags = re.IGNORECASE),\n",
    "                    value=' ',inplace=False,regex=True)\n",
    "        \n",
    "    def clean(self,text):\n",
    "        text = text.replace('\\r' , '')\n",
    "        text = text.replace('\\t' , '')\n",
    "        text = text.replace('?','').replace('-','')\n",
    "        text = text.replace('\\xa0',' ')\n",
    "        text = text.replace('\\n','SEPARATOR')\n",
    "        #text = re.sub('\\W' ,' ', text)\n",
    "        text =  re.sub('\\w+@\\w+.domain','email_set', text)\n",
    "        text = re.sub('<http://[\\w//.]+>' ,' url_set', text)\n",
    "        text = re.sub('http://[\\w.]+.com' ,'', text)\n",
    "        text = re.sub('<html[\\w/\\\"<>=., ]+' ,'', text)\n",
    "        text = re.sub('Disclaimer:[a-zA-Z0-9_,(). ]+' ,'', text)\n",
    "        text = re.sub('www.[\\w]+.com' ,'', text)\n",
    "    #        text = re.sub('Date:' ,'Sent:', text)\n",
    "        text = re.sub('CC:' ,'Cc:', text)\n",
    "        text = re.sub('Expéditeur:' ,'From:', text)\n",
    "        text = re.sub('De:' ,'From:', text)\n",
    "        text = re.sub('Enviado el:' ,'Sent:', text)\n",
    "        text = re.sub('Asunto:' ,'Subject:', text)\n",
    "        text = re.sub('Enviada em:' ,'Sent:', text)\n",
    "        text = re.sub('Assunto:' ,'Subject:', text)\n",
    "        \n",
    "        text = re.sub(' url_set' ,'', text)\n",
    "        text = text + ' From:'\n",
    "        text=re.sub(r'From:.*?(?=SEPARATOR)','THREADCHAINBREAK',text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_Non_Word(self,docs):\n",
    "        docs = re.sub(r'cid:email_set','',docs)\n",
    "        docs = re.sub(r'mailto:email_set','',docs)\n",
    "        docs = re.sub(r'email_set','',docs)\n",
    "        docs = re.sub(r'description:','',docs)\n",
    "        docs = re.sub(r'original message','',docs)\n",
    "        docs = re.sub(r'[\\*\\[\\]_<>/#|!=\\-\\&;\\?^\"\"]','',docs)\n",
    "        docs = re.sub(r'\\s+', ' ', docs)\n",
    "        if docs.startswith('clientname internal'):\n",
    "            docs = docs.replace('clientname internal' , '')\n",
    "        docs = re.sub(r\"\\'\", '', docs)\n",
    "        a = re.search(r'good (morning|afternoon)' , docs)\n",
    "        if a is not None: \n",
    "            docs = docs.replace(a.group(),'')\n",
    "        if docs.startswith('dear'):\n",
    "            docs = re.sub(r'dear','', docs)\n",
    "        docs = re.sub(r'(hi,|hello|hi )','',docs)\n",
    "        #docs = re.sub(r',',' ',docs)\n",
    "        docs = docs.strip()\n",
    "        return docs\n",
    "    \n",
    "    def remove_Repeated_Words(self,docs):\n",
    "        docs = re.sub('importance:|cc:|re:|:','',docs)\n",
    "        docs = re.sub('\\.+','.',docs)\n",
    "        docs = re.sub('SEPARATOR' , ' ',docs)\n",
    "        docs = re.sub('\\s+',' ',docs)\n",
    "        docs = docs.strip()\n",
    "        docs=docs.strip(' From')\n",
    "        return docs\n",
    "    \n",
    "    def salutations_regex(self):\n",
    "        \n",
    "        salutations=['Mit freundlichen Gruben','Thank you for your assistance','Thanks and Regards','Many thanks in advance','Thanks a lot',\n",
    "         'Thanks for the support','Thanks for your help','Thanks for all','Thank you for your support','Thanks in advance','Thanks so much','Thanks again',\n",
    "         'Thank you','Thanks you','Thanks & Regards','Many thanks','Best','Thank you in advance for your help',\n",
    "         'Thx','TKS','Thanks','Kind regards','Best regards','Cordialement','Saludos',\n",
    "         'Disclaimer','Sincerely','Brgds','With Regards','Krgds','Regards','Have a Wonderful Day',\n",
    "         'Please do not respond to this message','Visit the exciting new options','Rgds',\n",
    "         'Caution','This e-mail message and any attachment(s)','Please hurry']\n",
    "        \n",
    "        return '|'.join(salutations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CREATE DESCRIPTION PIPELINE\n",
    "pipeline_featureunion_list_desc = []\n",
    "##Part of config file\n",
    "body_extract_param = 'Pick_Full_Mail'\n",
    "##choices will be (‘pick_First_Mail’,’Pick_Words’,’ Pick_Full_Mail’)\n",
    "\n",
    "trim_words_param = 100\n",
    "\n",
    "if synonym_file_path!= '':    \n",
    "    with open(synonym_file_path, mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        syn_dict = {rows[0]:rows[1] for rows in reader}\n",
    "    pipeline_featureunion_list_desc.append(('content_desc', Pipeline([\n",
    "                                    ('selector', ItemSelector(key='Text.Body')),\n",
    "                                    ('preprocessing', BodyExtraction(body_extract=body_extract_param,set_num_words=trim_words_param)),\n",
    "                                    ('datetrns', DateTransformer()),\n",
    "                                    ('numtrns', NumberTransformer()),\n",
    "                                    ('puntrns', PunctTransformer()),\n",
    "                                    ('synonyms',SynonymTransformer(syn_dict)),\n",
    "                                    ('vect', CountVectorizer(stop_words = my_stop_words)),\n",
    "                                    ('tfidf', TfidfTransformer()),\n",
    "                            ])))\n",
    "else:\n",
    "    pipeline_featureunion_list_desc.append(('content_desc', Pipeline([\n",
    "                                        ('selector', ItemSelector(key='Text.Body')),\n",
    "                                        ('preprocessing', BodyExtraction(body_extract=body_extract_param,set_num_words=trim_words_param)),\n",
    "                                        ('datetrns', DateTransformer()),\n",
    "                                        ('numtrns', NumberTransformer()),\n",
    "                                        ('puntrns', PunctTransformer()),\n",
    "                                        ('vect', CountVectorizer(stop_words = my_stop_words)),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                ]))) \n",
    "pipeline_featureunion_list_desc.append(('content_stats', Pipeline([\n",
    "    ('selector', ItemSelector(key='Text.Body')),                            \n",
    "    ('stats', TextStats()),  # returns a list of dicts\n",
    "                                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                                ('scale',StandardScaler(with_mean=False)),\n",
    "                            ])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('content_desc', Pipeline(memory=None,\n",
       "       steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', C...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])),\n",
       " ('content_stats', Pipeline(memory=None,\n",
       "       steps=[('selector', ItemSelector(key='Text.Body')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "          sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]))]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_featureunion_list_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CREATE SUBJECT PIPELINE\n",
    "if 'Text.Subj' in data_df.columns.values:\n",
    "    if synonym_file_path!= '':    \n",
    "        with open(synonym_file_path, mode='r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            syn_dict = {rows[0]:rows[1] for rows in reader}\n",
    "        pipeline_featureunion_list_desc.append(('content_subj', Pipeline([\n",
    "                                        ('selector', ItemSelector(key='Text.Subj')),\n",
    "                                        ('preprocessing', BodyExtraction(body_extract=body_extract_param,set_num_words=trim_words_param)),\n",
    "                                        ('datetrns', DateTransformer()),\n",
    "                                        ('numtrns', NumberTransformer()),\n",
    "                                        ('puntrns', PunctTransformer()),\n",
    "                                        ('synonyms',SynonymTransformer(syn_dict)),\n",
    "                                        ('vect', CountVectorizer(stop_words = my_stop_words)),\n",
    "                                        ('tfidf', TfidfTransformer()),\n",
    "                                ])))\n",
    "    else:\n",
    "        pipeline_featureunion_list_desc.append(('content_subj', Pipeline([\n",
    "                                            ('selector', ItemSelector(key='Text.Subj')),\n",
    "                                            ('preprocessing', BodyExtraction(body_extract=body_extract_param,set_num_words=trim_words_param)),\n",
    "                                            ('datetrns', DateTransformer()),\n",
    "                                            ('numtrns', NumberTransformer()),\n",
    "                                            ('puntrns', PunctTransformer()),\n",
    "                                            ('vect', CountVectorizer(stop_words = my_stop_words)),\n",
    "                                            ('tfidf', TfidfTransformer()),\n",
    "                                    ]))) \n",
    "    pipeline_featureunion_list_desc.append(('content_sub_stats', Pipeline([\n",
    "        ('selector', ItemSelector(key='Text.Subj')),                            \n",
    "        ('stats', TextStats()),  # returns a list of dicts\n",
    "                                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                                    ('scale',StandardScaler(with_mean=False)),\n",
    "                                ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('content_desc', Pipeline(memory=None,\n",
       "       steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', C...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])),\n",
       " ('content_stats', Pipeline(memory=None,\n",
       "       steps=[('selector', ItemSelector(key='Text.Body')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "          sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))])),\n",
       " ('content_subj', Pipeline(memory=None,\n",
       "       steps=[('selector', ItemSelector(key='Text.Subj')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', C...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])),\n",
       " ('content_sub_stats', Pipeline(memory=None,\n",
       "       steps=[('selector', ItemSelector(key='Text.Subj')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "          sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]))]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_featureunion_list_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_features_mnb = Pipeline([('union', FeatureUnion(\n",
    "                        transformer_list=pipeline_featureunion_list_desc,\n",
    "                        )),\n",
    "                         #('scale',StandardScaler(with_mean=False)),\n",
    "                         ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\",dual=False))),\n",
    "                         ('clf', MultinomialNB())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('union', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('content_desc', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTra...prefit=False, threshold=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_features_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'union': FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('content_desc', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), (...rue,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]))],\n",
      "       transformer_weights=None), 'feature_selection': SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "        norm_order=1, prefit=False, threshold=None), 'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)}\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_features_mnb.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('union', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('content_desc', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), (...rue,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]))],\n",
      "       transformer_weights=None)), ('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "        norm_order=1, prefit=False, threshold=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))], 'union': FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('content_desc', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), (...rue,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]))],\n",
      "       transformer_weights=None), 'feature_selection': SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "        norm_order=1, prefit=False, threshold=None), 'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'union__n_jobs': 1, 'union__transformer_list': [('content_desc', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', C...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])), ('content_stats', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))])), ('content_subj', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Subj')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', C...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])), ('content_sub_stats', Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Subj')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]))], 'union__transformer_weights': None, 'union__content_desc': Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', C...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]), 'union__content_stats': Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Body')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]), 'union__content_subj': Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Subj')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', C...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]), 'union__content_sub_stats': Pipeline(memory=None,\n",
      "     steps=[('selector', ItemSelector(key='Text.Subj')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))]), 'union__content_desc__memory': None, 'union__content_desc__steps': [('selector', ItemSelector(key='Text.Body')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=frozenset({'upon', '', 'thin', 'urlhttpaffwww', 'beforehand', 'above', 'seeming', 'last', 'again', 'itself', 'keep', 'couldnt', 'etc', 'clientnamenow', 'cannot', 'side', 'comfdataponocordersclientnam', 'a', 'throughout', 'mostly', 'seems', 'afterwards', 'www', 'mailto', 'clientnam', 'neit...own', 'what', 'would', 'yet', 'ponopo', 'sometime', 'wwwclientnam', 'if', 'even', 'all', 'already'}),\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))], 'union__content_desc__selector': ItemSelector(key='Text.Body'), 'union__content_desc__preprocessing': BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100), 'union__content_desc__datetrns': DateTransformer(), 'union__content_desc__numtrns': NumberTransformer(), 'union__content_desc__puntrns': PunctTransformer(), 'union__content_desc__synonyms': SynonymTransformer(synonym_dict=None), 'union__content_desc__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=frozenset({'upon', '', 'thin', 'urlhttpaffwww', 'beforehand', 'above', 'seeming', 'last', 'again', 'itself', 'keep', 'couldnt', 'etc', 'clientnamenow', 'cannot', 'side', 'comfdataponocordersclientnam', 'a', 'throughout', 'mostly', 'seems', 'afterwards', 'www', 'mailto', 'clientnam', 'neit...own', 'what', 'would', 'yet', 'ponopo', 'sometime', 'wwwclientnam', 'if', 'even', 'all', 'already'}),\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'union__content_desc__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'union__content_desc__selector__key': 'Text.Body', 'union__content_desc__preprocessing__body_extract': 'Pick_Full_Mail', 'union__content_desc__preprocessing__set_num_words': 100, 'union__content_desc__synonyms__synonym_dict': None, 'union__content_desc__vect__analyzer': 'word', 'union__content_desc__vect__binary': False, 'union__content_desc__vect__decode_error': 'strict', 'union__content_desc__vect__dtype': <class 'numpy.int64'>, 'union__content_desc__vect__encoding': 'utf-8', 'union__content_desc__vect__input': 'content', 'union__content_desc__vect__lowercase': True, 'union__content_desc__vect__max_df': 1.0, 'union__content_desc__vect__max_features': None, 'union__content_desc__vect__min_df': 1, 'union__content_desc__vect__ngram_range': (1, 1), 'union__content_desc__vect__preprocessor': None, 'union__content_desc__vect__stop_words': frozenset({'upon', '', 'thin', 'urlhttpaffwww', 'beforehand', 'above', 'seeming', 'last', 'again', 'itself', 'keep', 'couldnt', 'etc', 'clientnamenow', 'cannot', 'side', 'comfdataponocordersclientnam', 'a', 'throughout', 'mostly', 'seems', 'afterwards', 'www', 'mailto', 'clientnam', 'neither', 'please', 'several', 'mine', 'your', 'move', 'which', 'their', 'every', 'https', 'call', 'we', 'whereas', 'thereupon', 'describe', 'whom', 'fifteen', 'these', 'nowhere', 'anyway', 'fire', 'empty', 'domain', 'anyhow', 'so', 'could', 'where', 'wclientnam', 'although', 'everywhere', 'order ', 'otherwise', 'see', 'wherein', 'anything', 'ushttp', 'noone', 'each', 'fifty', 'too', 'forty', 'therefore', 'due', 'here', 'de', 'among', 'everything', 'whoever', 'un', 'whatever', 'amongst', 'mill', 'very', 'were', 'down', 'will', 'his', 'latter', 'but', 'six', 'third', 'one', 'had', 'thereby', 'nevertheless', 'eleven', 'during', 'its', 'nine', 'herein', 'in', 'thankschip', 'july', 'monday', 'they', 'while', 'however', 'via', 'wherever', 'somewhere', 'once', 'sincere', 'of', 'can', 'it', 'someemailaddressdomain', 'became', 'interest', 'well', 'my', 'have', 'seem', 'enough', 'cant', 'from', 'give', 'over', 'email', 'tuesday', 'none', 'whenever', 'plea', 'html', 'hundred', 'ltd', 'sixty', 'find', 'ten', 'often', 'under', 'into', 'anyone', 'casepr ', 'full', 'ie', 'either', 'i', 'get', 'becoming', 'much', 'thus', 'yours', 'must', 'almost', 'be', 'back', 'only', 'aerohttp', 'someemailaddress', 'both', 'how', 'hers', 'along', 'whereafter', 'cry', 'quotationpr ', 'between', 'hereafter', 'is', 'another', 'about', 'twelve', 'not', 'or', 'wednesday', 'thru', 'least', 'indeed', 'beyond', 'per', 'httpsaerospaceclientnamecomencontactus', 'still', 'behind', 'beside', 'thereafter', 'might', 'nobody', 'name', 'req ', 'and', 'something', 'other', 'httpsaerospaceclientnamecom', 'should', 'themselves', 'thick', 'latterly', 'hasnt', 'some', 'you', 'clientnamecom', 'five', 'an', 'urlhttp', 'below', 'show', 'before', 'first', 'formerly', 'whence', 'herself', 'amoungst', 'saturday', 'four', 'pleas', 'tosomeemailaddressdomain', 'towards', 'fill', 'done', 'amount', 'few', 'yourselves', 'rather', 'onto', 'to', 'take', 'are', 'until', 'such', 'sunday', 'meanwhile', 'whether', 'becomes', 'now', 'sometimes', 'someemailaddressclientname', 'many', 'because', 'being', 'for', 'the', 'hence', 'total ', 'who', 'against', 'next', 'less', 'may', 'thence', 'ponopono', 'everyone', 'bill', 'ourselves', 'top', 'jan', 'two', 'cidsomeemailaddressdomain', 'without', 'has', 'himself', 'toward', 'somehow', 'ours', 'found', 'do', 'con', 'though', 'nothing', 're', 'therein', 'never', 'am', 'hereby', 'mail', 'go', 'part', 'whose', 'pono', 'august', 'elsewhere', 'system', 'as', 'ever', 'ref ', 'around', 'whereby', 'made', 'myself', 'june', 'front', 'someone', 'there', 'mailtosomeemailaddressdomain', 'seemed', 'when', 'and ', 'off', 'since', 'eight', 'bottom', 'them', 'after', 'out', 'clientnamenowne', 'than', 'http', 'friday', 'by', 'hereupon', 'yourself', 'together', 'most', 'thursday', 'others', 'put', 'also', 'comhttp', 'across', 'no', 'mobilepr ', 'namely', 'that', 'those', 'former', 'perhaps', 'anywhere', 'except', 'was', 'with', 'inc', 'moreover', 'then', 'at', 'serious', 'through', 'else', 'alone', 'on', 'clientname', 'why', 'whole', 'whereupon', 'us', 'nor', 'more', 'become', 'twenty', 'further', 'same', 'clientnameaerone', 'our', 'she', 'him', 'up', 'he', 'besides', 'within', 'co', 'eg', 'her', 'whither', 'any', 'always', 'been', 'detail', 'this', 'me', 'three', 'clientnameaerohttp', 'own', 'what', 'would', 'yet', 'ponopo', 'sometime', 'wwwclientnam', 'if', 'even', 'all', 'already'}), 'union__content_desc__vect__strip_accents': None, 'union__content_desc__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'union__content_desc__vect__tokenizer': None, 'union__content_desc__vect__vocabulary': None, 'union__content_desc__tfidf__norm': 'l2', 'union__content_desc__tfidf__smooth_idf': True, 'union__content_desc__tfidf__sublinear_tf': False, 'union__content_desc__tfidf__use_idf': True, 'union__content_stats__memory': None, 'union__content_stats__steps': [('selector', ItemSelector(key='Text.Body')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))], 'union__content_stats__selector': ItemSelector(key='Text.Body'), 'union__content_stats__stats': TextStats(), 'union__content_stats__vect': DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True), 'union__content_stats__scale': StandardScaler(copy=True, with_mean=False, with_std=True), 'union__content_stats__selector__key': 'Text.Body', 'union__content_stats__vect__dtype': <class 'numpy.float64'>, 'union__content_stats__vect__separator': '=', 'union__content_stats__vect__sort': True, 'union__content_stats__vect__sparse': True, 'union__content_stats__scale__copy': True, 'union__content_stats__scale__with_mean': False, 'union__content_stats__scale__with_std': True, 'union__content_subj__memory': None, 'union__content_subj__steps': [('selector', ItemSelector(key='Text.Subj')), ('preprocessing', BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100)), ('datetrns', DateTransformer()), ('numtrns', NumberTransformer()), ('puntrns', PunctTransformer()), ('synonyms', SynonymTransformer(synonym_dict=None)), ('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=frozenset({'upon', '', 'thin', 'urlhttpaffwww', 'beforehand', 'above', 'seeming', 'last', 'again', 'itself', 'keep', 'couldnt', 'etc', 'clientnamenow', 'cannot', 'side', 'comfdataponocordersclientnam', 'a', 'throughout', 'mostly', 'seems', 'afterwards', 'www', 'mailto', 'clientnam', 'neit...own', 'what', 'would', 'yet', 'ponopo', 'sometime', 'wwwclientnam', 'if', 'even', 'all', 'already'}),\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))], 'union__content_subj__selector': ItemSelector(key='Text.Subj'), 'union__content_subj__preprocessing': BodyExtraction(body_extract='Pick_Full_Mail', set_num_words=100), 'union__content_subj__datetrns': DateTransformer(), 'union__content_subj__numtrns': NumberTransformer(), 'union__content_subj__puntrns': PunctTransformer(), 'union__content_subj__synonyms': SynonymTransformer(synonym_dict=None), 'union__content_subj__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=frozenset({'upon', '', 'thin', 'urlhttpaffwww', 'beforehand', 'above', 'seeming', 'last', 'again', 'itself', 'keep', 'couldnt', 'etc', 'clientnamenow', 'cannot', 'side', 'comfdataponocordersclientnam', 'a', 'throughout', 'mostly', 'seems', 'afterwards', 'www', 'mailto', 'clientnam', 'neit...own', 'what', 'would', 'yet', 'ponopo', 'sometime', 'wwwclientnam', 'if', 'even', 'all', 'already'}),\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'union__content_subj__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'union__content_subj__selector__key': 'Text.Subj', 'union__content_subj__preprocessing__body_extract': 'Pick_Full_Mail', 'union__content_subj__preprocessing__set_num_words': 100, 'union__content_subj__synonyms__synonym_dict': None, 'union__content_subj__vect__analyzer': 'word', 'union__content_subj__vect__binary': False, 'union__content_subj__vect__decode_error': 'strict', 'union__content_subj__vect__dtype': <class 'numpy.int64'>, 'union__content_subj__vect__encoding': 'utf-8', 'union__content_subj__vect__input': 'content', 'union__content_subj__vect__lowercase': True, 'union__content_subj__vect__max_df': 1.0, 'union__content_subj__vect__max_features': None, 'union__content_subj__vect__min_df': 1, 'union__content_subj__vect__ngram_range': (1, 1), 'union__content_subj__vect__preprocessor': None, 'union__content_subj__vect__stop_words': frozenset({'upon', '', 'thin', 'urlhttpaffwww', 'beforehand', 'above', 'seeming', 'last', 'again', 'itself', 'keep', 'couldnt', 'etc', 'clientnamenow', 'cannot', 'side', 'comfdataponocordersclientnam', 'a', 'throughout', 'mostly', 'seems', 'afterwards', 'www', 'mailto', 'clientnam', 'neither', 'please', 'several', 'mine', 'your', 'move', 'which', 'their', 'every', 'https', 'call', 'we', 'whereas', 'thereupon', 'describe', 'whom', 'fifteen', 'these', 'nowhere', 'anyway', 'fire', 'empty', 'domain', 'anyhow', 'so', 'could', 'where', 'wclientnam', 'although', 'everywhere', 'order ', 'otherwise', 'see', 'wherein', 'anything', 'ushttp', 'noone', 'each', 'fifty', 'too', 'forty', 'therefore', 'due', 'here', 'de', 'among', 'everything', 'whoever', 'un', 'whatever', 'amongst', 'mill', 'very', 'were', 'down', 'will', 'his', 'latter', 'but', 'six', 'third', 'one', 'had', 'thereby', 'nevertheless', 'eleven', 'during', 'its', 'nine', 'herein', 'in', 'thankschip', 'july', 'monday', 'they', 'while', 'however', 'via', 'wherever', 'somewhere', 'once', 'sincere', 'of', 'can', 'it', 'someemailaddressdomain', 'became', 'interest', 'well', 'my', 'have', 'seem', 'enough', 'cant', 'from', 'give', 'over', 'email', 'tuesday', 'none', 'whenever', 'plea', 'html', 'hundred', 'ltd', 'sixty', 'find', 'ten', 'often', 'under', 'into', 'anyone', 'casepr ', 'full', 'ie', 'either', 'i', 'get', 'becoming', 'much', 'thus', 'yours', 'must', 'almost', 'be', 'back', 'only', 'aerohttp', 'someemailaddress', 'both', 'how', 'hers', 'along', 'whereafter', 'cry', 'quotationpr ', 'between', 'hereafter', 'is', 'another', 'about', 'twelve', 'not', 'or', 'wednesday', 'thru', 'least', 'indeed', 'beyond', 'per', 'httpsaerospaceclientnamecomencontactus', 'still', 'behind', 'beside', 'thereafter', 'might', 'nobody', 'name', 'req ', 'and', 'something', 'other', 'httpsaerospaceclientnamecom', 'should', 'themselves', 'thick', 'latterly', 'hasnt', 'some', 'you', 'clientnamecom', 'five', 'an', 'urlhttp', 'below', 'show', 'before', 'first', 'formerly', 'whence', 'herself', 'amoungst', 'saturday', 'four', 'pleas', 'tosomeemailaddressdomain', 'towards', 'fill', 'done', 'amount', 'few', 'yourselves', 'rather', 'onto', 'to', 'take', 'are', 'until', 'such', 'sunday', 'meanwhile', 'whether', 'becomes', 'now', 'sometimes', 'someemailaddressclientname', 'many', 'because', 'being', 'for', 'the', 'hence', 'total ', 'who', 'against', 'next', 'less', 'may', 'thence', 'ponopono', 'everyone', 'bill', 'ourselves', 'top', 'jan', 'two', 'cidsomeemailaddressdomain', 'without', 'has', 'himself', 'toward', 'somehow', 'ours', 'found', 'do', 'con', 'though', 'nothing', 're', 'therein', 'never', 'am', 'hereby', 'mail', 'go', 'part', 'whose', 'pono', 'august', 'elsewhere', 'system', 'as', 'ever', 'ref ', 'around', 'whereby', 'made', 'myself', 'june', 'front', 'someone', 'there', 'mailtosomeemailaddressdomain', 'seemed', 'when', 'and ', 'off', 'since', 'eight', 'bottom', 'them', 'after', 'out', 'clientnamenowne', 'than', 'http', 'friday', 'by', 'hereupon', 'yourself', 'together', 'most', 'thursday', 'others', 'put', 'also', 'comhttp', 'across', 'no', 'mobilepr ', 'namely', 'that', 'those', 'former', 'perhaps', 'anywhere', 'except', 'was', 'with', 'inc', 'moreover', 'then', 'at', 'serious', 'through', 'else', 'alone', 'on', 'clientname', 'why', 'whole', 'whereupon', 'us', 'nor', 'more', 'become', 'twenty', 'further', 'same', 'clientnameaerone', 'our', 'she', 'him', 'up', 'he', 'besides', 'within', 'co', 'eg', 'her', 'whither', 'any', 'always', 'been', 'detail', 'this', 'me', 'three', 'clientnameaerohttp', 'own', 'what', 'would', 'yet', 'ponopo', 'sometime', 'wwwclientnam', 'if', 'even', 'all', 'already'}), 'union__content_subj__vect__strip_accents': None, 'union__content_subj__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'union__content_subj__vect__tokenizer': None, 'union__content_subj__vect__vocabulary': None, 'union__content_subj__tfidf__norm': 'l2', 'union__content_subj__tfidf__smooth_idf': True, 'union__content_subj__tfidf__sublinear_tf': False, 'union__content_subj__tfidf__use_idf': True, 'union__content_sub_stats__memory': None, 'union__content_sub_stats__steps': [('selector', ItemSelector(key='Text.Subj')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)), ('scale', StandardScaler(copy=True, with_mean=False, with_std=True))], 'union__content_sub_stats__selector': ItemSelector(key='Text.Subj'), 'union__content_sub_stats__stats': TextStats(), 'union__content_sub_stats__vect': DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True), 'union__content_sub_stats__scale': StandardScaler(copy=True, with_mean=False, with_std=True), 'union__content_sub_stats__selector__key': 'Text.Subj', 'union__content_sub_stats__vect__dtype': <class 'numpy.float64'>, 'union__content_sub_stats__vect__separator': '=', 'union__content_sub_stats__vect__sort': True, 'union__content_sub_stats__vect__sparse': True, 'union__content_sub_stats__scale__copy': True, 'union__content_sub_stats__scale__with_mean': False, 'union__content_sub_stats__scale__with_std': True, 'feature_selection__estimator__C': 1.0, 'feature_selection__estimator__class_weight': None, 'feature_selection__estimator__dual': False, 'feature_selection__estimator__fit_intercept': True, 'feature_selection__estimator__intercept_scaling': 1, 'feature_selection__estimator__loss': 'squared_hinge', 'feature_selection__estimator__max_iter': 1000, 'feature_selection__estimator__multi_class': 'ovr', 'feature_selection__estimator__penalty': 'l1', 'feature_selection__estimator__random_state': None, 'feature_selection__estimator__tol': 0.0001, 'feature_selection__estimator__verbose': 0, 'feature_selection__estimator': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0), 'feature_selection__norm_order': 1, 'feature_selection__prefit': False, 'feature_selection__threshold': None, 'clf__alpha': 1.0, 'clf__class_prior': None, 'clf__fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_features_mnb.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_mnb = pipeline_features_mnb.fit(X_cat, Y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predefined PArameters to be part of config file\n",
    "N_FEATURES_OPTIONS = [2, 4, 8]\n",
    "N_FEATURES_chi = [10, 20, 30]\n",
    "C_OPTIONS = [1, 10, 100, 1000]\n",
    "NUMBER_OF_ESTIMATORS_RF = [50, 80, 100]\n",
    "stknb_alpha_param = [0.1, 1e-2]\n",
    "ngram_range_param_subj = [(1, 2), (1, 3)]\n",
    "ngram_range_param_desc = [(1, 2), (1, 3),(2,3)]\n",
    "alpha_param = [1, 0.1, 1e-2]\n",
    "true_false_param = [True, False]\n",
    "log_C = (1.0, 10.0)\n",
    "max_df_param = (0.25, 0.5, 0.75)\n",
    "min_df_param = (0.01,0.02,0.03)\n",
    "\n",
    "##penalty param\n",
    "##L1_C1 = [0.01, 0.02, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hyperparameter tuning step\n",
    "if 'Text.Subj' in data_df.columns.values:\n",
    "    parameters_nb = [{\n",
    "                            'union__content__desc__vect':  [CountVectorizer(stop_words = my_stop_words),\n",
    "                                         StemmedCountVectorizer(stop_words = my_stop_words)],\n",
    "                            'union__content__desc__vect__ngram_range': ngram_range_param_desc,\n",
    "                            'union__content__desc__vect__max_df': max_df_param,\n",
    "                            'union__content__desc__vect__min_df': min_df_param,\n",
    "                            'union__content__desc__tfidf__use_idf': true_false_param,\n",
    "                            'union__content__subj__vect':  [CountVectorizer(stop_words = my_stop_words),\n",
    "                                         StemmedCountVectorizer(stop_words = my_stop_words)],\n",
    "                            'union__content__subj__vect__ngram_range': ngram_range_param_subj,\n",
    "                            'union__content__subj__vect__max_df': max_df_param,\n",
    "                            'union__content__subj__vect__min_df': min_df_param,\n",
    "                            'union__content__subj__tfidf__use_idf': true_false_param,\n",
    "                            'clf__alpha': alpha_param\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectPercentile(chi2)],\n",
    "                            'feature_selection__percentile': N_FEATURES_chi\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectFromModel(ExtraTreesClassifier())],\n",
    "                            'feature_selection__estimator__n_estimators' : NUMBER_OF_ESTIMATORS_RF\n",
    "\n",
    "                        },\n",
    "                        {   'feature_selection': [SelectPercentile(f_classif)],\n",
    "                            'feature_selection__percentile': N_FEATURES_chi\n",
    "\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectFromModel(LinearSVC(penalty=\"l1\",dual=False, C =0.01))],\n",
    "                        }]\n",
    "else:\n",
    "    parameters_nb = [{\n",
    "                            'union__content__desc__vect':  [CountVectorizer(stop_words = my_stop_words),\n",
    "                                         StemmedCountVectorizer(stop_words = my_stop_words)],\n",
    "                            'union__content__desc__vect__ngram_range': ngram_range_param_desc,\n",
    "                            'union__content__desc__vect__max_df': max_df_param,\n",
    "                            'union__content__desc__vect__min_df': min_df_param,\n",
    "                            'union__content__desc__tfidf__use_idf': true_false_param,\n",
    "                            'clf__alpha': alpha_param\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectPercentile(chi2)],\n",
    "                            'feature_selection__percentile': N_FEATURES_chi\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectFromModel(ExtraTreesClassifier())],\n",
    "                            'feature_selection__estimator__n_estimators' : NUMBER_OF_ESTIMATORS_RF\n",
    "\n",
    "                        },\n",
    "                        {   'feature_selection': [SelectPercentile(f_classif)],\n",
    "                            'feature_selection__percentile': N_FEATURES_chi\n",
    "\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectFromModel(LinearSVC(penalty=\"l1\",dual=False, C =0.01))],\n",
    "                        }]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###instance of the grid search by passing the classifier, parameters \n",
    "# and n_jobs= -1 which tells to use multiple cores from user machine.\n",
    "gs_clf_mnb = GridSearchCV(text_clf_mnb, parameters_nb, n_jobs= -1,verbose=1, scoring = f_scorer,\n",
    "                          cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=32))\n",
    "\n",
    "gs_clf_mnb = gs_clf_mnb.fit(X_cat, Y_cat)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Multinomial Naive Bayes estimators\n",
    "mod1 = gs_clf_mnb.best_estimator_\n",
    "# single model stacking with NB\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###logistic model\n",
    "text_clf_logr = Pipeline([('union', FeatureUnion(\n",
    "                            transformer_list=pipeline_featureunion_list_desc,\n",
    "                        )),\n",
    "                             #('scale',StandardScaler(with_mean=False)),\n",
    "                             ('reduce_dim',SelectFromModel(LinearSVC(penalty=\"l1\",dual=False))),\n",
    "                             ('logistic', LogisticRegression(multi_class=‘multinomial’))\n",
    "                                                  \n",
    "                                                   ])\n",
    "text_clf_logr = text_clf_logr.fit(X_cat, Y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Text.Subj' in initial_file.columns.values:\n",
    "    parameters_logr = [{\n",
    "                            'union__content__desc__vect':  [CountVectorizer(stop_words = my_stop_words),\n",
    "                                         StemmedCountVectorizer(stop_words = my_stop_words)],\n",
    "                            'union__content__desc__vect__ngram_range': ngram_range_param_desc,\n",
    "                            'union__content__desc__vect__max_df': max_df_param,\n",
    "                            'union__content__desc__vect__min_df': min_df_param,\n",
    "                            'union__content__desc__tfidf__use_idf': true_false_param,\n",
    "                            'union__content__subj__vect':  [CountVectorizer(stop_words = my_stop_words),\n",
    "                                         StemmedCountVectorizer(stop_words = my_stop_words)],\n",
    "                            'union__content__subj__vect__ngram_range': ngram_range_param_subj,\n",
    "                            'union__content__subj__vect__max_df': max_df_param\n",
    "                            'union__content__subj__vect__min_df': min_df_param \n",
    "                            'union__content__subj__tfidf__use_idf': true_false_param,\n",
    "                            'logistic__C': log_C\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectPercentile(chi2)],\n",
    "                            'feature_selection__percentile': N_FEATURES_chi\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectFromModel(ExtraTreesClassifier())],\n",
    "                            'feature_selection__estimator__n_estimators' : NUMBER_OF_ESTIMATORS_RF\n",
    "\n",
    "                        },\n",
    "                        {   'feature_selection': [SelectPercentile(f_classif)],\n",
    "                            'feature_selection__percentile': N_FEATURES_chi\n",
    "\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectFromModel(LinearSVC(penalty=\"l1\",dual=False, C =0.01))],\n",
    "                        }]\n",
    "else:\n",
    "    parameters_logr = [{\n",
    "                            'union__content__desc__vect':  [CountVectorizer(stop_words = my_stop_words),\n",
    "                                         StemmedCountVectorizer(stop_words = my_stop_words)],\n",
    "                            'union__content__desc__vect__ngram_range': ngram_range_param_desc,\n",
    "                            'union__content__desc__vect__max_df': max_df_param,\n",
    "                            'union__content__desc__vect__min_df': min_df_param,\n",
    "                            'union__content__desc__tfidf__use_idf': true_false_param,\n",
    "                            'logistic__C': log_C\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectPercentile(chi2)],\n",
    "                            'feature_selection__percentile': N_FEATURES_chi\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectFromModel(ExtraTreesClassifier())],\n",
    "                            'feature_selection__estimator__n_estimators' : NUMBER_OF_ESTIMATORS_RF\n",
    "\n",
    "                        },\n",
    "                        {   'feature_selection': [SelectPercentile(f_classif)],\n",
    "                            'feature_selection__percentile': N_FEATURES_chi\n",
    "\n",
    "                        },\n",
    "                        {\n",
    "                            'feature_selection': [SelectFromModel(LinearSVC(penalty=\"l1\",dual=False, C =0.01))],\n",
    "                        }]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_logr = GridSearchCV(text_clf_logr, parameters_logr, n_jobs=-1,verbose = 1, scoring = f_scorer,\n",
    "                              cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = gs_clf_logr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Stacking of the models Naive Bayes and Logistic Regression\n",
    "text_stack_mnb = Pipeline([('union', FeatureUnion(transformer_list=[\n",
    "                            ('mod1', MyModelTransformer(mod1)),\n",
    "                            ('mod2',MyModelTransformer(mod2))])),\n",
    "                         ('stkmnb',MultinomialNB())])\n",
    "text_stack_mnb = text_stack_mnb.fit(X_cat, Y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Parameters for Stacking\n",
    "parameters_stk_nb = {\n",
    "                                'union__mod1':[MyModelTransformer(mod1)],\n",
    "                                'union__mod2':[MyModelTransformer(mod2)],\n",
    "                                 'stkmnb__alpha': stknb_alpha_param\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##grid search on Stacking\n",
    "gs_stack_mnb = GridSearchCV(text_stack_mnb, parameters_stk_nb, n_jobs=-1,verbose=1, scoring = f_scorer,\n",
    "                          cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=32))\n",
    "gs_stack_mnb = gs_stack_mnb.fit(X_cat, Y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Results of Grid search Stacked Model \n",
    "print(\"Results for Grid Search Stacked Models\")\n",
    "print(\"Best Accuracy for Grid Search Stacked Models\")\n",
    "print(gs_stack_mnb.best_score_)\n",
    "print(\"Best Params for Grid Search Stacked Models\")\n",
    "print(gs_stack_mnb.best_params_)\n",
    "        \n",
    "print(\"Results for Grid Search Stacked Models on validation data\")\n",
    "predicted_nb_gs = gs_stack_mnb.best_estimator_.predict(validation[desc_col[0]])\n",
    "print(\"Accuracy for Grid Search Stacked Models\")\n",
    "print(np.mean(predicted_nb_gs == validation[target_col]))\n",
    "print(\"precision_recall_fscore_support for GS Stacked NB\")\n",
    "print(precision_recall_fscore_support(validation[target_col], predicted_nb_gs, average='weighted'))\n",
    "#Classfication report for Grid Search Stacked Models\n",
    "cr_df = classifaction_report_df(classification_report(validation[target_col], predicted_nb_gs))\n",
    "cr_df.to_csv(output_dir + '/' + portfolio_name + \\\n",
    "                     '_' + \\\n",
    "                     'validation_classification_report_mulstk_' + \\\n",
    "                     now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                               index  = True)\n",
    "#Confusion matrix for Grid search Stacked Model\n",
    "cf_df = confusion_matrix_df(validation[target_col], predicted_nb_gs)\n",
    "cf_df.to_csv(output_dir + '/' + portfolio_name + \\\n",
    "                     '_validation_confusion_matrix_mulstk_' + \\\n",
    "                     now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                               index  = False)\n",
    "mod1_features = get_used_features(mod1)\n",
    "mod2_features = get_used_features(mod2)\n",
    "used_features = list(set().union(mod1_features,mod2_features))\n",
    "used_features = sorted(used_features)\n",
    "with open(output_dir + '/' + portfolio_name + \\\n",
    "                     '_used_features_mulstk_' + \\\n",
    "                     now.strftime(\"%Y-%m-%d\") + \".csv\",'w', newline = '') as resultFile:\n",
    "wr = csv.writer(resultFile, dialect='excel')\n",
    "#wr.writerow(all_features)\n",
    "for item in used_features:\n",
    "        wr.writerow((item, ))\n",
    "grid_result_df = get_grid_values(gs_clf_svm)\n",
    "grid_result_df = grid_result_df.sort_values(['means', 'stds'], ascending=[False, True])\n",
    "grid_result_df.to_csv(output_dir + '/' + portfolio_name + \\\n",
    "                 '_grid_results_svm_' + \\\n",
    "                 now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                 index  = False )\n",
    "with open(output_dir + '/' + portfolio_name + \\\n",
    "                     '_saved_grid_search_object_mulstk_' + \\\n",
    "                     now.strftime(\"%Y-%m-%d\") + \".pkl\", 'wb') as f:  \n",
    "    pickle.dump([gs_clf_mnb, gs_clf_mnb.best_estimator_], f)\n",
    "        \n",
    "print(\"Results for Grid search Stacked Model on test\")\n",
    "predicted_nb_gs = gs_stack_mnb.best_estimator_.predict(test[desc_col[0]])\n",
    "print(\"Accuracy for Grid search Stacked Model\")\n",
    "print(np.mean(predicted_nb_gs == test[target_col]))\n",
    "print(\"precision_recall_fscore_support for GS Stacked NB\")\n",
    "print(precision_recall_fscore_support(test[target_col], predicted_nb_gs, average='weighted'))\n",
    "#Classfication report for Grid search Stacked Model\n",
    "cr_df = classifaction_report_df(classification_report(test[target_col], predicted_nb_gs))\n",
    "cr_df.to_csv(output_dir + '/' + portfolio_name + \\\n",
    "                     '_test_classification_report_mulstk_' + \\\n",
    "                     now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                               index  = True)\n",
    "#Confusion matrix for Grid search Stacked Model\n",
    "cf_df = confusion_matrix_df(test[target_col], predicted_nb_gs)\n",
    "cf_df.to_csv(output_dir + '/' + portfolio_name + \\\n",
    "                     '_test_confusion_matrix_mulstk_' + \\\n",
    "                     now.strftime(\"%Y-%m-%d\") + \".csv\",\n",
    "                               index  = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
