{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "training_file = 'D:/WorkbenchDir/iris_new/train_val1_partial_masked_2018-03-08.csv'\n",
    "validation_file = 'D:/WorkbenchDir/iris_new/validation2_partial_masked_2018-03-08.csv'\n",
    "stop_words_file = 'D:/WorkbenchDir/iris_new/stop_words_not_masked.csv'\n",
    "synonym_file_path = 'D:/WorkbenchDir/iris_new/synonyms.csv'\n",
    "domain_keyword_file_path = 'D:/WorkbenchDir/iris_new/domain_key_words.csv'\n",
    "dump_file_location = 'D:/WorkbenchDir/iris_new/email_classification_multiclass_approach1_obj.pkl'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.metrics import precision_recall_fscore_support,confusion_matrix,classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.metrics import make_scorer,f1_score\n",
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#import csv\n",
    "import string\n",
    "from scipy.sparse import lil_matrix, find\n",
    "import itertools\n",
    "##from pyjarowinkler import distance\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "import os.path\n",
    "import hashlib\n",
    "import pickle\n",
    "import filelock\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_df):\n",
    "        return data_df[self.key]\n",
    "    \n",
    "    def get_feature_names():\n",
    "       return []\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': (text.count('.') + \\\n",
    "                                   text.count('?') + \\\n",
    "                                   text.count('!'))}\n",
    "                for text in posts]\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return ['length','num_sentences']\n",
    "\n",
    "class TargetSimilarity(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, target,stop_words = None, ngram_range = (1,3),use_idf = False):\n",
    "        self.target = target\n",
    "        self.stop_words = stop_words\n",
    "        self.ngram_range = ngram_range\n",
    "        self.use_idf = use_idf\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text):\n",
    "        text_target = np.append(text,self.target)\n",
    "        count_vect = StemmedCountVectorizer(stop_words = self.stop_words, \n",
    "                                     ngram_range = self.ngram_range)\n",
    "        counts = count_vect.fit_transform(text_target)\n",
    "        \n",
    "        # TF-IDF\n",
    "        tfidf_transformer = TfidfTransformer(use_idf = self.use_idf)\n",
    "        tfidf = tfidf_transformer.fit_transform(counts)\n",
    "        #tfidf = TfidfVectorizer().fit_transform(text_target)\n",
    "        #cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "        cosine_similarities = (tfidf * tfidf.T).A\n",
    "        #squareform(pdist(tfidf.toarray(), 'cosine'))\n",
    "        return cosine_similarities[:-len(self.target),len(text):]\n",
    "    \n",
    "class MyModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "class NumberTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        num_text_col = text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))[0-9][0-9.,\\-\\/]*(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "                    value='NUMBERSPECIALTOKEN',inplace=False,regex=True)\n",
    "        return num_text_col\n",
    "\n",
    "# =============================================================================\n",
    "#         text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))[0-9][0-9.,\\-\\/]*(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "#                     value='NUMBERSPECIALTOKEN',inplace=True,regex=True)\n",
    "#         return text_col\n",
    "# =============================================================================\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "# =============================================================================\n",
    "#         text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))(\\d+[/-]\\d+[/-]\\d+)(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "#                 value='DATESPECIALTOKEN',inplace=True,regex=True)\n",
    "# =============================================================================\n",
    "        date_text_col = text_col.replace(to_replace=re.compile('(?:(?<=\\s)|(?<=^)|(?<=[^0-9a-zA-Z]))(\\d+[/-]\\d+[/-]\\d+)(?:(?=\\s)|(?=$)|(?=[^0-9a-zA-Z]))',flags = re.IGNORECASE),\n",
    "                 value='DATESPECIALTOKEN',inplace=False,regex=True)\n",
    "        return date_text_col\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "   \n",
    "class SynonymTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def __init__(self, synonym_dict):\n",
    "        self.syn_dict = synonym_dict\n",
    "        #print(self.syn_dict)\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        #print(self.syn_dict)\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        #print(self.syn_dict)\n",
    "        date_text_col = text_col.replace(self.syn_dict,regex=True)\n",
    "        return date_text_col\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "\n",
    "class PunctTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        regexp = '['+string.punctuation+']{2,}'\n",
    "        punct_text_col = text_col.replace(to_replace=re.compile(regexp,\n",
    "                                                                flags = re.IGNORECASE),\n",
    "                                             value='',inplace=False,regex=True)\n",
    "        return punct_text_col\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "       return None\n",
    "\n",
    "    \n",
    "class FeaturizeDomainKeyWords(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def __init__(self, domain_keyword_list = []):\n",
    "        self.keywords_list = domain_keyword_list\n",
    "        #print(self.keywords_list)\n",
    "    \n",
    "    def fit(self, x, y = None):\n",
    "        #print(self.keywords_list)\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_col):\n",
    "        #print(self.keywords_list)\n",
    "        keyword_textcol_list = []\n",
    "        if self.keywords_list != None:\n",
    "            for text in text_col:\n",
    "                keyword_textcol_dict = {}\n",
    "                for keywords in self.keywords_list:\n",
    "                    if(len(keywords) > 1):\n",
    "                        keyword_reg = \"|\".join(keywords)\n",
    "                    else:\n",
    "                        keyword_reg = keywords[0]\n",
    "                    keyword = 'has_keyword_' + keywords[0]\n",
    "                    keyword_textcol_dict[keyword] = bool(re.search(keyword_reg,text,re.IGNORECASE))\n",
    "                keyword_textcol_list.append(keyword_textcol_dict)\n",
    "        return keyword_textcol_list\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "        keyword_col_list = ['has_keyword_' + keywords[0] for keywords in self.keywords_list]\n",
    "        return keyword_col_list\n",
    "    \n",
    "def ClassDiscriminatingMeasure(X,y):\n",
    "    CDM_tk =np.zeros(shape=(X.shape[1],))\n",
    "#full_term_sum = tr_dsc_vect.tocsr().sum(0)\n",
    "    for category in np.unique(y):\n",
    "        #print(category)\n",
    "        pos_loc = np.where(y == category)[0]\n",
    "        cat_num_doc = len(pos_loc)\n",
    "        #print(cat_num_doc)\n",
    "        neg_loc = np.where(y != category)[0]\n",
    "        neg_cat_num_doc = len(neg_loc)\n",
    "        #print(neg_cat_num_doc)\n",
    "        cat_term = X.tocsr()[pos_loc,:]\n",
    "        #(nonzero_rows,nonzero_cols,_)=sparse.find(cat_term)\n",
    "        tk_ci = np.diff(cat_term.tocsc().indptr)\n",
    "        P_tk_ci = tk_ci / cat_num_doc\n",
    "        #cat_term_sum = cat_term.sum(0)\n",
    "        cat_term_neg = X.tocsr()[neg_loc,:]\n",
    "        #cat_term_neg_sum = cat_term_neg.sum(0)\n",
    "        #(nonzero_rows,nonzero_cols)=cat_term_neg.nonzero()\n",
    "        tk_neg_ci = np.diff(cat_term_neg.tocsc().indptr)\n",
    "        P_tk_neg_ci = (1 + tk_neg_ci)/ neg_cat_num_doc\n",
    "        CDM_tk_ci = np.log1p(P_tk_ci/P_tk_neg_ci)\n",
    "        CDM_tk = CDM_tk + CDM_tk_ci\n",
    "    #print(CDM_tk.shape)\n",
    "    return CDM_tk\n",
    "\n",
    "def get_context_d_tk_w(d, tk, w = 3,token_regex = r\"(?u)\\b\\w\\w+\\b\"):\n",
    "    #sentence = sentence.split()\n",
    "    #d = re.split('[\\s\\-\\:]+',d)\n",
    "    r_splt = re.compile(token_regex)\n",
    "    d = r_splt.findall(d)\n",
    "    len_d = len(d)\n",
    "    tk = tk.split()\n",
    "    num_words = len(tk)\n",
    "    r_st = re.compile(r\"\\b%s\\b\" % tk[0], re.IGNORECASE|re.MULTILINE)\n",
    "    r_cmp = re.compile(r\"\\b%s\\b\" % ' '.join(tk), re.IGNORECASE|re.MULTILINE)\n",
    "    for i,word in enumerate(d):\n",
    "        if bool(r_st.match(word)) and \\\n",
    "        bool(r_cmp.match(' '.join(d[i:i+num_words]))):\n",
    "            #print(i)\n",
    "            #print(word)\n",
    "            begin_pad = []\n",
    "            end_pad = []\n",
    "            if (i-w < 0):\n",
    "                for b in reversed(range(0,w-i)):\n",
    "                    begin_pad.append('__START_'+ str(b) +'__')\n",
    "            #print(begin_pad)\n",
    "            if (i+num_words+w > len_d):\n",
    "                for e in range(0,i+num_words+w - len_d):\n",
    "                    end_pad.append('__END_'+ str(e) +'__')\n",
    "            #print(end_pad)\n",
    "            start = max(0, i-w)\n",
    "            #print(d[start:i+num_words+w])\n",
    "            begin_pad.extend(d[start:i+num_words+w])\n",
    "            #print(begin_pad)\n",
    "            begin_pad.extend(end_pad)\n",
    "            yield ' '.join(begin_pad)\n",
    "\n",
    "def pairs(*lists):\n",
    "    for t in itertools.combinations(lists, 2):\n",
    "        for pair in itertools.product(*t):\n",
    "            yield pair\n",
    "            \n",
    "def get_sim_context_d_tk_w(docs, tk, m_w = 3):\n",
    "    sim_context_all_w = []\n",
    "    for w in reversed(range(0,m_w + 1)):\n",
    "        doc_contexts = []\n",
    "        doc_contexts_itr = docs.apply(get_context_d_tk_w,args = (tk,w))\n",
    "        doc_context_num = []\n",
    "        for context in doc_contexts_itr:\n",
    "            list_context = list(context)\n",
    "            doc_context_num.append(len(list_context)) \n",
    "            doc_contexts.append(list_context)\n",
    "        sim_context_w = []\n",
    "        for x in pairs(*doc_contexts):\n",
    "            sim_context_w.append(distance.get_jaro_distance(x[0],x[1]))\n",
    "        sim_context_all_w.append(sim_context_w)\n",
    "    sim_context_all_w = np.asarray(sim_context_all_w)\n",
    "    sim_context_all_w = sim_context_all_w.sum(0)/ (m_w + 1)\n",
    "    #range_list = []\n",
    "    sim_context_d_tk_w = []\n",
    "    for i in range(len(doc_context_num)):\n",
    "        cr = 0\n",
    "        range_list = []\n",
    "        for pair in itertools.combinations(list(range(len(doc_context_num))),2): \n",
    "            #print(pair)\n",
    "            last_pos = cr + (doc_context_num[pair[0]] * doc_context_num[pair[1]])\n",
    "            all_pos = list(range(cr,last_pos))\n",
    "            #print(all_pos)\n",
    "            cr = last_pos      \n",
    "            if i in pair:\n",
    "                range_list.extend(all_pos)\n",
    "        #range_list.append(tmp_range_list)\n",
    "        sim_context_d_tk_w.append(sum(sim_context_all_w[range_list]))\n",
    "    return(sim_context_d_tk_w)\n",
    "       \n",
    "def ClassDiscriminatingMeasureCS(X,y):\n",
    "    CDM_tk =np.zeros(shape=(X.shape[1],))\n",
    "#full_term_sum = tr_dsc_vect.tocsr().sum(0)\n",
    "    for category in np.unique(y):\n",
    "        #print(category)\n",
    "        pos_loc = np.where(y == category)[0]\n",
    "        cat_num_doc = len(pos_loc)\n",
    "        #print(cat_num_doc)\n",
    "        neg_loc = np.where(y != category)[0]\n",
    "        neg_cat_num_doc = len(neg_loc)\n",
    "        #print(neg_cat_num_doc)\n",
    "        cat_term = X.tocsr()[pos_loc,:]\n",
    "        #(nonzero_rows,nonzero_cols,_)=sparse.find(cat_term)\n",
    "        tk_ci = cat_term.sum(0)\n",
    "        P_tk_ci = tk_ci / cat_num_doc\n",
    "        #cat_term_sum = cat_term.sum(0)\n",
    "        cat_term_neg = X.tocsr()[neg_loc,:]\n",
    "        #cat_term_neg_sum = cat_term_neg.sum(0)\n",
    "        #(nonzero_rows,nonzero_cols)=cat_term_neg.nonzero()\n",
    "        tk_neg_ci = cat_term_neg.sum(0)\n",
    "        P_tk_neg_ci = (1 + tk_neg_ci)/ neg_cat_num_doc\n",
    "        CDM_tk_ci = np.log1p(P_tk_ci/P_tk_neg_ci)\n",
    "        CDM_tk = CDM_tk + CDM_tk_ci\n",
    "    #print((CDM_tk.A1).shape)\n",
    "    return  CDM_tk.A1\n",
    "\n",
    "def get_sim_context_tk_w(terms,\n",
    "                       count_vect_obj,\n",
    "                       raw_document,\n",
    "                       max_window = 3,\n",
    "                       token_regex = r\"(?u)\\b\\w\\w+\\b\",\n",
    "                       stop_words = None,\n",
    "                       cache_dir = None):\n",
    "                                           #'(?u)\\\\b\\\\w\\\\w+\\\\b'):\n",
    "    cache_dict = {}\n",
    "    is_cache = False\n",
    "    cache_update = False\n",
    "    if (cache_dir != None and os.path.isdir(cache_dir)):\n",
    "        is_cache = True\n",
    "        file_sign_str = (raw_document.str.cat(sep = ' ') + str(max_window)).encode(encoding = 'utf-8')\n",
    "        hash_object = hashlib.md5(file_sign_str)\n",
    "        cache_file_path = cache_dir + '/' + hash_object.hexdigest() + '.pkl'\n",
    "        if os.path.isfile(cache_file_path):\n",
    "            with open (cache_file_path, 'rb') as fp:\n",
    "                cache_dict = pickle.load(fp)\n",
    "    term_list = count_vect_obj.get_feature_names()\n",
    "    raw_document.index = range(len(raw_document))\n",
    "    #r_splt = re.compile(\"%s\" % token_regex)\n",
    "    data_lower = raw_document.str.lower().str.findall(token_regex)\n",
    "    if stop_words != None:\n",
    "        data_lower_stop = data_lower.apply(lambda x: ' '.join([item for item in x if item not in stop_words]))\n",
    "    else:\n",
    "        data_lower_stop = data_lower\n",
    "    nz_rows, nz_cols, nz_val = find(terms) #.nonzero()\n",
    "    num_terms = terms.shape[1]\n",
    "    ret_mat = lil_matrix(terms.shape)\n",
    "    for term_idx in range(0,(num_terms-1)):\n",
    "        #print(term_idx)\n",
    "        term_doc_indx = nz_rows[np.where(nz_cols == term_idx)[0]]\n",
    "        #nz_val[np.where(nz_cols == term_idx)[0]]\n",
    "        if (len(term_doc_indx) == 1):\n",
    "            ret_mat[term_doc_indx,term_idx] = 0 # should this be 1 instead as unique term\n",
    "        else:\n",
    "           tk =  term_list[term_idx]\n",
    "           docs = data_lower_stop[term_doc_indx]\n",
    "           if len(cache_dict) > 0 and tk in cache_dict:\n",
    "               sim_context_d_tk_w = cache_dict[tk]\n",
    "           else:    \n",
    "               sim_context_d_tk_w = get_sim_context_d_tk_w(docs,tk,max_window)\n",
    "               if is_cache:\n",
    "                   cache_dict[tk] = sim_context_d_tk_w\n",
    "                   cache_update = True\n",
    "           for i,row_idx in enumerate(term_doc_indx):\n",
    "               ret_mat[row_idx,term_idx] = sim_context_d_tk_w[i]\n",
    "    \n",
    "    if is_cache and cache_update:\n",
    "        lock = filelock.FileLock(\"{}.lock\".format(cache_file_path))\n",
    "        try:\n",
    "            with lock.acquire(timeout = 10):\n",
    "                with open(cache_file_path, 'wb') as fp:\n",
    "                    pickle.dump(cache_dict, fp)\n",
    "        except lock.Timeout:\n",
    "            print('update_cache timeout' + cache_file_path)\n",
    "    #CDM_tk = ClassDiscriminatingMeasure(ret_mat,y,'sum')\n",
    "    return ret_mat\n",
    "\n",
    "\n",
    "class ContextSimilarityBasedFeatureSelection(CountVectorizer):\n",
    "    def __init__(self,max_window = 3,\n",
    "                 input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None,\n",
    "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), analyzer='word',\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 vocabulary=None, binary=False, dtype=np.int64,\n",
    "                 percentile = 10,cache_dir = None):\n",
    "        super(ContextSimilarityBasedFeatureSelection, self).__init__(input,\n",
    "             encoding, decode_error, strip_accents, lowercase , preprocessor,\n",
    "             tokenizer, stop_words, token_pattern, ngram_range ,\n",
    "             analyzer, max_df, min_df, max_features, vocabulary, binary, \n",
    "             dtype)\n",
    "        self.max_window = max_window\n",
    "        self.token_pattern = token_pattern\n",
    "        self.stop_words = stop_words\n",
    "        #self.percentile = percentile\n",
    "        self._red_dim = SelectPercentile(score_func=ClassDiscriminatingMeasureCS,\n",
    "                                         percentile = percentile)\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "    @property\n",
    "    def percentile(self):\n",
    "        return self._red_dim.percentile\n",
    "\n",
    "    @percentile.setter\n",
    "    def percentile(self, value):\n",
    "        self._red_dim.percentile = value\n",
    "    \n",
    "    @property\n",
    "    def score_func(self):\n",
    "        return self._red_dim.score_func\n",
    "\n",
    "    @score_func.setter\n",
    "    def score_func(self, value):\n",
    "        self._red_dim.score_func = value\n",
    "# =============================================================================\n",
    "#     def fit(self, raw_documents, y=None):\n",
    "#         return self\n",
    "# =============================================================================\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        dtm = super(ContextSimilarityBasedFeatureSelection, self).fit_transform(raw_documents)\n",
    "        sim_context_tk_w = get_sim_context_tk_w(terms = dtm,\n",
    "                       count_vect_obj = super(ContextSimilarityBasedFeatureSelection, self),\n",
    "                       raw_document = raw_documents,\n",
    "                       max_window = self.max_window,\n",
    "                       token_regex = self.token_pattern,\n",
    "                       stop_words = self.stop_words,\n",
    "                       cache_dir = self.cache_dir)\n",
    "        self._red_dim.fit_transform(sim_context_tk_w,y)\n",
    "        self.selected_cols = self._red_dim.get_support(indices=True)\n",
    "        return dtm[:,self.selected_cols]\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        #check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')\n",
    "        dtm = super(ContextSimilarityBasedFeatureSelection, self).transform(raw_documents)\n",
    "        return dtm[:,self.selected_cols]\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        all_features = super(ContextSimilarityBasedFeatureSelection, self).get_feature_names()\n",
    "        return np.asarray(all_features)[self.selected_cols]\n",
    "\n",
    "def classifaction_report_df(report):\n",
    "    report = re.sub(r\" +\", \" \", report).replace(\"avg / total\", \"avg/total\").replace(\"\\n \", \"\\n\")\n",
    "    report_df = pd.read_csv(StringIO(\"Classes\" + report), sep=' ', index_col=0)        \n",
    "    return(report_df)\n",
    "# =============================================================================\n",
    "#     report_data = []\n",
    "#     lines = report.split('\\n')\n",
    "#     for line in lines[2:-3]:\n",
    "#         row = {}\n",
    "#         row_data = line.split('      ')\n",
    "#         row['class'] = row_data[0]\n",
    "#         row['precision'] = float(row_data[1])\n",
    "#         row['recall'] = float(row_data[2])\n",
    "#         row['f1_score'] = float(row_data[3])\n",
    "#         row['support'] = float(row_data[4])\n",
    "#         report_data.append(row)\n",
    "#     dataframe = pd.DataFrame.from_dict(report_data)\n",
    "# #    dataframe.to_csv('classification_report.csv', index = False)\n",
    "#     return dataframe\n",
    "# =============================================================================\n",
    "\n",
    "def get_used_features(mod,explicit_feature_selection = True):\n",
    "    if explicit_feature_selection:\n",
    "        mod_support = mod.named_steps['reduce_dim'].get_support(indices=True)\n",
    "    features = []\n",
    "    for trnf_list in mod.named_steps['union'].transformer_list:\n",
    "        features.extend(trnf_list[1].named_steps['vect'].get_feature_names())\n",
    "    if explicit_feature_selection:\n",
    "        return(np.asarray(features)[mod_support])\n",
    "    else:\n",
    "        return(np.asarray(features))\n",
    "\n",
    "def get_grid_values(gs_obj):\n",
    "    means = gs_obj.cv_results_['mean_test_score']\n",
    "    stds = gs_obj.cv_results_['std_test_score']\n",
    "    col_name = ['means', 'stds']\n",
    "    col_name.extend(list(gs_obj.cv_results_['params'][0].keys()))\n",
    "    perf_df = pd.DataFrame(columns=col_name )    \n",
    "    i = 0   \n",
    "    for mean, std, params in zip(means, stds, gs_obj.cv_results_['params']):\n",
    "# =============================================================================\n",
    "#         print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#                   % (mean, std * 2, params))\n",
    "# =============================================================================\n",
    "        row_list = [mean,std * 2]   \n",
    "        row_list.extend(params.values())\n",
    "        perf_df.loc[i] = row_list\n",
    "        i += 1\n",
    "    return perf_df\n",
    "\n",
    "def confusion_matrix_df(y_actu,y_pred):\n",
    "    y_actu = pd.Series(y_actu, name='Actual')\n",
    "    y_pred = pd.Series(y_pred, name='Predicted')\n",
    "    #return np.array2string(confusion_matrix, separator=', ')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred, \n",
    "                               rownames=['Actual'], colnames=['Predicted'], \n",
    "                               margins=True)\n",
    "    return df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'D:/WorkbenchDir/iris_new/train_val1_partial_masked_2018-03-08.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d2607d45d1bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cp1252'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#text in column 1, classifier in column 2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m###Clean Data in Subject and Description\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Subject'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Subject'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m training['Subject'].replace(to_replace=re.compile('[x]{2,}',flags = re.IGNORECASE),\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'D:/WorkbenchDir/iris_new/train_val1_partial_masked_2018-03-08.csv' does not exist"
     ]
    }
   ],
   "source": [
    "training = pd.read_csv(training_file,encoding='cp1252') #text in column 1, classifier in column 2.\n",
    "###Clean Data in Subject and Description\n",
    "training['Subject'].replace(' ', np.nan, inplace=True)\n",
    "training.dropna(subset=['Subject'], inplace=True)\n",
    "training['Subject'].replace(to_replace=re.compile('[x]{2,}',flags = re.IGNORECASE),\n",
    " value=' ',inplace=True,regex=True)\n",
    "\n",
    "training['Description'].replace(' ', np.nan, inplace=True)\n",
    "training.dropna(subset=['Description'], inplace=True)\n",
    "training['Description'].replace(to_replace=re.compile('[x]{2,}',flags = re.IGNORECASE),\n",
    "value=' ',inplace=True,regex=True)\n",
    "##Clean Data in Subject and Description\n",
    "validation = pd.read_csv(validation_file,encoding='cp1252') #text in column 1, classifier in column 2.\n",
    "validation['Subject'].replace(' ', np.nan, inplace=True)\n",
    "validation.dropna(subset=['Subject'], inplace=True)\n",
    "validation['Subject'].replace(to_replace=re.compile('[x]{2,}',flags = re.IGNORECASE),\n",
    "value=' ',inplace=True,regex=True)\n",
    "validation['Description'].replace(' ', np.nan, inplace=True)\n",
    "validation.dropna(subset=['Description'], inplace=True)\n",
    "validation['Description'].replace(to_replace=re.compile('[x]{2,}',flags = re.IGNORECASE),\n",
    "value=' ',inplace=True,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
